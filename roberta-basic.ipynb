{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94915d97-0f63-4805-b4a3-66e94b3c5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "os.chdir(\"/g/data/jr19/rh2942/text-empathy/\")\n",
    "from evaluation import pearsonr\n",
    "from utils.utils import plot, get_device, set_all_seeds\n",
    "from utils.common import EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf579713-872e-41d6-8a8c-8f14a6e50498",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # due to huggingface warning\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ccd8e2-f398-46e3-8c29-b1a6c858c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(RobertaRegressor, self).__init__()\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "    ):\n",
    "\n",
    "        output = self.transformer(\n",
    "            input_ids= input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee314462-a714-4cea-80bb-82097bb625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, task, checkpoint, batch_size, feature_to_tokenise, seed):\n",
    "\n",
    "        self.task = task\n",
    "        self.checkpoint = checkpoint\n",
    "        self.batch_size = batch_size\n",
    "        self.tokeniser = AutoTokenizer.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokeniser)\n",
    "        self.feature_to_tokenise = feature_to_tokenise # to tokenise function\n",
    "        self.seed = seed\n",
    "\n",
    "        assert len(self.task) == 1, 'task must be a list with one element'\n",
    "    \n",
    "    def _process_raw(self, path, send_label):\n",
    "        data = pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "        if send_label:\n",
    "            text = data[self.feature_to_tokenise + self.task]\n",
    "        else:\n",
    "            text = data[self.feature_to_tokenise]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _tokeniser_fn(self, sentence):\n",
    "        if len(self.feature_to_tokenise) == 1: # only one feature\n",
    "            return self.tokeniser(sentence[self.feature_to_tokenise[0]], truncation=True)\n",
    "        # otherwise tokenise a pair of sentence\n",
    "        return self.tokeniser(sentence[self.feature_to_tokenise[0]], sentence[self.feature_to_tokenise[1]], truncation=True)\n",
    "\n",
    "    def _process_input(self, file, send_label):\n",
    "        data = self._process_raw(path=file, send_label=send_label)\n",
    "        data = Dataset.from_pandas(data, preserve_index=False) # convert to huggingface dataset\n",
    "        data = data.map(self._tokeniser_fn, batched=True, remove_columns=self.feature_to_tokenise) # tokenise\n",
    "        data = data.with_format('torch')\n",
    "        return data\n",
    "\n",
    "\n",
    "    # taken from https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    def _seed_worker(self, worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)     \n",
    "\n",
    "    def dataloader(self, file, send_label, shuffle):\n",
    "        data = self._process_input(file=file, send_label=send_label)\n",
    "\n",
    "        # making sure the shuffling is reproducible\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.seed)\n",
    "        \n",
    "        return DataLoader(\n",
    "            data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=24,\n",
    "            worker_init_fn=self._seed_worker,\n",
    "            generator=g\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095ee97b-ccaf-410a-bd26-5067564e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, task, model, lr, n_epochs, train_loader,\n",
    "                 dev_loader, dev_label_file, device_id=0):\n",
    "        self.device = get_device(device_id)\n",
    "        self.task = task\n",
    "        self.model = model.to(self.device)\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.train_loader = train_loader\n",
    "        self.dev_loader = dev_loader\n",
    "        self.dev_label_file = dev_label_file\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimiser = torch.optim.AdamW(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-06,\n",
    "            weight_decay=0.1\n",
    "        )\n",
    "\n",
    "        n_training_step = self.n_epochs*len(self.train_loader)\n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=self.optimiser,\n",
    "            num_warmup_steps=0.06*n_training_step,\n",
    "            num_training_steps=n_training_step\n",
    "        )\n",
    "        \n",
    "        self.best_pearson_r = -1.0 # initiliasation\n",
    "        self.early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n",
    "        \n",
    "        assert len(self.task) == 1, 'task must be a list with one element'\n",
    "\n",
    "    def _training_step(self):\n",
    "        tr_loss = 0.0\n",
    "        self.model.train()\n",
    "    \n",
    "        for data in self.train_loader:\n",
    "            input_ids = data['input_ids'].to(self.device, dtype=torch.long)\n",
    "            attention_mask = data['attention_mask'].to(self.device, dtype=torch.long)\n",
    "            targets = data[self.task[0]].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            loss = self.loss_fn(outputs.logits, targets)\n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "            self.optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "        epoch_loss = tr_loss / len(train_loader)\n",
    "        print(f'Train loss: {epoch_loss}')\n",
    "\n",
    "    def fit(self, save_model=False):\n",
    "        dev_label = pd.read_csv(self.dev_label_file, sep='\\t', header=None)\n",
    "        if self.task[0] == 'empathy':\n",
    "            true = dev_label.iloc[:, 0].tolist()\n",
    "        if self.task[0] == 'distress':\n",
    "            true = dev_label.iloc[:, 1].tolist()\n",
    "            \n",
    "        for epoch in range(self.n_epochs):\n",
    "            print(f'Epoch: {epoch+1}')\n",
    "            self._training_step()\n",
    "\n",
    "            preds = self.evaluate(dataloader=self.dev_loader, load_model=False)\n",
    "            \n",
    "            pearson_r = pearsonr(true, preds)\n",
    "            print(f'Pearson r: {pearson_r}')\n",
    "            \n",
    "            val_loss = self.loss_fn(torch.tensor(preds), torch.tensor(true))\n",
    "            print('Validation loss:', val_loss.item())\n",
    "            \n",
    "            if self.early_stopper.early_stop(val_loss):\n",
    "                break\n",
    "                \n",
    "            if (pearson_r > self.best_pearson_r):\n",
    "                self.best_pearson_r = pearson_r            \n",
    "                if save_model:\n",
    "                    torch.save(self.model.state_dict(), 'roberta-baseline.pth')\n",
    "                    print(\"Saved the model in epoch \" + str(epoch+1))\n",
    "            \n",
    "            print(f'Best dev set Pearson r: {self.best_pearson_r}\\n')\n",
    "\n",
    "    def evaluate(self, dataloader, load_model=False):\n",
    "        if load_model:\n",
    "            self.model.load_state_dict(torch.load('roberta-baseline.pth'))\n",
    "    \n",
    "        pred = torch.empty((len(dataloader.dataset), 1), device=self.device) # len(self.dev_loader.dataset) --> # of samples\n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            for data in dataloader:\n",
    "                input_ids = data['input_ids'].to(self.device, dtype=torch.long)\n",
    "                attention_mask = data['attention_mask'].to(self.device, dtype=torch.long)\n",
    "        \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "\n",
    "                batch_size = outputs.logits.shape[0]\n",
    "                pred[idx:idx+batch_size, :] = outputs.logits\n",
    "                idx += batch_size\n",
    "            \n",
    "        return [float(k) for k in pred]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e65bb5-f377-4664-8720-b18276ee77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'roberta-base'\n",
    "task = ['empathy']\n",
    "# feature_to_tokenise=['demographic_essay', 'article']\n",
    "feature_to_tokenise=['demographic', 'essay']\n",
    "seed = 0\n",
    "\n",
    "# train_file = './data/essay-train-ws22-ws23.tsv'\n",
    "train_file = './data/PREPROCESSED-WS22-WS23-train.tsv'\n",
    "# train_file = './data/COMBINED-PREPROCESSED-PARAPHRASED-WS22-WS23-train.tsv'\n",
    "\n",
    "# WASSA 2022\n",
    "# dev_file = './data/PREPROCESSED-WS22-dev.tsv'\n",
    "# dev_label_file = './data/WASSA22/goldstandard_dev_2022.tsv'\n",
    "# test_file = './data/PREPROCESSED-WS22-test.tsv'\n",
    "\n",
    "# WASSA 2023\n",
    "dev_file = './data/PREPROCESSED-WS23-dev.tsv'\n",
    "dev_label_file = './data/WASSA23/goldstandard_dev.tsv'\n",
    "test_file = './data/PREPROCESSED-WS23-test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc69ec6b-9bda-44f5-90b4-d02754f65d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 8.830813059662328\n",
      "Pearson r: 0.314\n",
      "Validation loss: 2.711336851119995\n",
      "Saved the model in epoch 1\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 3.1766385475794476\n",
      "Pearson r: 0.397\n",
      "Validation loss: 2.446990489959717\n",
      "Saved the model in epoch 2\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 2.5060301087119363\n",
      "Pearson r: 0.525\n",
      "Validation loss: 2.1250340938568115\n",
      "Saved the model in epoch 3\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 2.0900013956156642\n",
      "Pearson r: 0.555\n",
      "Validation loss: 2.07834792137146\n",
      "Saved the model in epoch 4\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 1.6857742378205964\n",
      "Pearson r: 0.56\n",
      "Validation loss: 2.173739433288574\n",
      "Saved the model in epoch 5\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 1.3553416967391967\n",
      "Pearson r: 0.551\n",
      "Validation loss: 2.592956781387329\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 1.132301505948558\n",
      "Pearson r: 0.542\n",
      "Validation loss: 3.2954039573669434\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds(seed)\n",
    "\n",
    "data_module = DataModule(\n",
    "    task=task,\n",
    "    checkpoint=checkpoint,\n",
    "    batch_size=16,\n",
    "    feature_to_tokenise=feature_to_tokenise,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "train_loader = data_module.dataloader(file=train_file, send_label=True, shuffle=True)\n",
    "dev_loader = data_module.dataloader(file=dev_file, send_label=False, shuffle=False)\n",
    "test_loader = data_module.dataloader(file=test_file, send_label=False, shuffle=False)\n",
    "\n",
    "model = RobertaRegressor(checkpoint=checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    lr=1e-5,\n",
    "    n_epochs=10,\n",
    "    train_loader=train_loader,\n",
    "    dev_loader=dev_loader,\n",
    "    dev_label_file=dev_label_file,\n",
    "    device_id=0\n",
    ")\n",
    "\n",
    "trainer.fit(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15c9c9-d937-4f8a-825f-7ef384dad0f3",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d5d6d3-b64d-4ffa-80bb-e20306380e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emp</th>\n",
       "      <th>dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.229848</td>\n",
       "      <td>4.229848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.446956</td>\n",
       "      <td>5.446956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.469219</td>\n",
       "      <td>5.469219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.797584</td>\n",
       "      <td>4.797584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.095503</td>\n",
       "      <td>5.095503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.177078</td>\n",
       "      <td>5.177078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.093154</td>\n",
       "      <td>5.093154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2.943219</td>\n",
       "      <td>2.943219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.054709</td>\n",
       "      <td>5.054709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.990912</td>\n",
       "      <td>3.990912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emp       dis\n",
       "0   4.229848  4.229848\n",
       "1   5.446956  5.446956\n",
       "2   5.469219  5.469219\n",
       "3   4.797584  4.797584\n",
       "4   5.095503  5.095503\n",
       "..       ...       ...\n",
       "95  5.177078  5.177078\n",
       "96  5.093154  5.093154\n",
       "97  2.943219  2.943219\n",
       "98  5.054709  5.054709\n",
       "99  3.990912  3.990912\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = trainer.evaluate(dataloader=test_loader, load_model=True)\n",
    "pred_df = pd.DataFrame({'emp': pred, 'dis': pred}) # we're not predicting distress, just aligning with submission system\n",
    "pred_df.to_csv('./tmp/predictions_EMP.tsv', sep='\\t', index=None, header=None)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f66af-5e0d-48a0-a37f-28db0804321f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
