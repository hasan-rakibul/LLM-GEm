{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "94915d97-0f63-4805-b4a3-66e94b3c5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "os.chdir(\"/g/data/jr19/rh2942/text-empathy/\")\n",
    "from evaluation import pearsonr\n",
    "from utils.utils import plot, get_device, set_all_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf579713-872e-41d6-8a8c-8f14a6e50498",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # due to huggingface warning\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee314462-a714-4cea-80bb-82097bb625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule():\n",
    "    def __init__(self, task, checkpoint, batch_size, feature_to_tokenise):\n",
    "        super(DataModule, self).__init__()\n",
    "        self.task = task\n",
    "        self.checkpoint = checkpoint\n",
    "        self.batch_size = batch_size\n",
    "        self.tokeniser = AutoTokenizer.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokeniser)\n",
    "        self.feature_to_tokenise = feature_to_tokenise # to tokenise function\n",
    "        assert len(self.feature_to_tokenise) == 1, 'feature_to_tokenise must be a list with one element'\n",
    "    \n",
    "    def _process_raw(self, path, send_label):\n",
    "        data = pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "        if send_label:\n",
    "            text = data[self.feature_to_tokenise + self.task]\n",
    "        else:\n",
    "            text = data[self.feature_to_tokenise]\n",
    "\n",
    "        demog = ['gender', 'education', 'race', 'age', 'income']        \n",
    "        data_demog = data[demog]\n",
    "        scaler = MinMaxScaler()\n",
    "        data_demog = pd.DataFrame(\n",
    "            scaler.fit_transform(data_demog),\n",
    "            columns=demog\n",
    "        )\n",
    "        data = pd.concat([text, data_demog], axis=1) \n",
    "            \n",
    "        return data\n",
    "\n",
    "    def _tokeniser_fn(self, sentence):\n",
    "        assert len(self.feature_to_tokenise) == 1 # only one feature\n",
    "        return self.tokeniser(sentence[self.feature_to_tokenise[0]], truncation=True)\n",
    "\n",
    "    def _process_input(self, file, send_label):\n",
    "        data = self._process_raw(path=file, send_label=send_label)\n",
    "        data = data.reset_index(drop=True)\n",
    "        data = Dataset.from_pandas(data, preserve_index=False) # convert to huggingface dataset\n",
    "        data = data.map(self._tokeniser_fn, batched=True, remove_columns=self.feature_to_tokenise) # tokenise\n",
    "        data = data.with_format('torch')\n",
    "        return data\n",
    "\n",
    "    # taken from https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    def _seed_worker(self, worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)     \n",
    "\n",
    "    def dataloader(self, file, send_label, shuffle):\n",
    "        data = self._process_input(file=file, send_label=send_label)\n",
    "\n",
    "        # making sure the shuffling is reproducible\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(0)\n",
    "        \n",
    "        return DataLoader(\n",
    "            data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=24,\n",
    "            worker_init_fn=self._seed_worker,\n",
    "            generator=g\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "96ccd8e2-f398-46e3-8c29-b1a6c858c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(BiEncoder, self).__init__()\n",
    "        self.transformer_article = AutoModel.from_pretrained(checkpoint, output_hidden_states=True, add_pooling_layer=False)\n",
    "        self.transformer_essay = AutoModel.from_pretrained(checkpoint, output_hidden_states=True, add_pooling_layer=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # self.fc1 = nn.Linear(1538, 768) #two hidden states (each of 768) and two similarity scores\n",
    "        self.fc1 = nn.Linear(6151, 768) #two hidden states (each of 3072) and two similarity scores\n",
    "        \n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.final = nn.Linear(256, 1)\n",
    "\n",
    "    def _get_embeddings(self, output):\n",
    "        layers = [-4, -3, -2, -1] #last four hidden states\n",
    "        states = output.hidden_states\n",
    "        embedding = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "        # print(embedding.shape)\n",
    "        return embedding[:, 0, :] #return only CLS token's embedding\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_article=None,\n",
    "        attention_mask_article=None,\n",
    "        input_ids_essay=None,\n",
    "        attention_mask_essay=None,\n",
    "        gender=None,\n",
    "        education=None,\n",
    "        race=None,\n",
    "        age=None,\n",
    "        income=None\n",
    "    ):\n",
    "\n",
    "        output_article = self.transformer_article(\n",
    "            input_ids=input_ids_article,\n",
    "            attention_mask=attention_mask_article\n",
    "        )\n",
    "        \n",
    "        embedding_article = self._get_embeddings(output_article)\n",
    "        \n",
    "        ## Using last hidden state only\n",
    "        # output_article = output_article.last_hidden_state\n",
    "        # output_article = output_article[:, 0] # CLS token. shape: (batch_size,768)\n",
    "\n",
    "        ## Using last four hidden states\n",
    "        output_article = output_article.hidden_states\n",
    "        output_article = torch.cat([output_article[i] for i in [-4, -3, -2, -1]], dim=-1) #shape: (batch_size, seq_length, 768*4)\n",
    "        output_article = output_article[:, 0] #CLS token. shape: (batch_size, 768*4)\n",
    "\n",
    "        ### Essay\n",
    "\n",
    "        output_essay = self.transformer_essay(\n",
    "            input_ids=input_ids_essay,\n",
    "            attention_mask=attention_mask_essay\n",
    "        )\n",
    "        embedding_essay = self._get_embeddings(output_essay)\n",
    "        \n",
    "        ## Using last hidden state only\n",
    "        # output_essay = output_essay.last_hidden_state\n",
    "        # output_essay = output_essay[:, 0]\n",
    "\n",
    "        ## Using last four hidden states\n",
    "        output_essay = output_essay.hidden_states\n",
    "        output_essay = torch.cat([output_essay[i] for i in [-4, -3, -2, -1]], dim=-1) #shape: (batch_size, seq_length, 768*4)\n",
    "        output_essay = output_essay[:, 0] #CLS token\n",
    "\n",
    "        cosine = F.cosine_similarity(embedding_article, embedding_essay, dim=1).view(-1, 1) #shape: (batch_size, 1)\n",
    "        euclidean = F.pairwise_distance(embedding_article, embedding_essay).view(-1, 1) #shape: (batch_size, 1)\n",
    "\n",
    "        output = torch.cat((output_article, output_essay, cosine, euclidean,\n",
    "                           gender, education, race, age, income), dim=1) # shape: (batch_size, 3072*2+1+5)\n",
    "        \n",
    "        output = self.fc1(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.fc3(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.final(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "095ee97b-ccaf-410a-bd26-5067564e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, task, model, lr, n_epoch, train_loader_article, train_loader_essay,\n",
    "                dev_loader_article, dev_loader_essay, dev_label_file):\n",
    "        self.device = get_device(0)\n",
    "        \n",
    "        self.task = task\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.n_epoch = n_epoch\n",
    "        self.train_loader_article = train_loader_article\n",
    "        self.train_loader_essay = train_loader_essay\n",
    "        self.dev_loader_article = dev_loader_article\n",
    "        self.dev_loader_essay = dev_loader_essay\n",
    "        self.dev_label_file = dev_label_file\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        self.optimiser = torch.optim.AdamW(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-06,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "\n",
    "        n_training_step = self.n_epoch*len(self.train_loader_article)\n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=self.optimiser,\n",
    "            num_warmup_steps=0.01*n_training_step,\n",
    "            num_training_steps=n_training_step\n",
    "        )\n",
    "        \n",
    "        self.best_pearson_r = -1.0 # initiliasation\n",
    "        \n",
    "        assert len(self.task) == 1, 'Task must be a list with one element'\n",
    "\n",
    "    def _training_step(self, epoch):\n",
    "        tr_loss = 0.0\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "        for (data_article, data_essay) in zip(self.train_loader_article, self.train_loader_essay):\n",
    "            input_ids_article = data_article['input_ids'].to(self.device, dtype=torch.long)\n",
    "            attention_mask_article = data_article['attention_mask'].to(self.device, dtype=torch.long)\n",
    "            input_ids_essay = data_essay['input_ids'].to(self.device, dtype=torch.long)\n",
    "            attention_mask_essay = data_essay['attention_mask'].to(self.device, dtype=torch.long)\n",
    "\n",
    "            assert (data_article[self.task[0]].detach().numpy() == data_essay[self.task[0]].detach().numpy()).all(), 'Ground truth is different between encoders'\n",
    "            targets = data_article[self.task[0]].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            \n",
    "            gender = data_article['gender'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            education = data_article['education'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            race = data_article['race'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            age = data_article['age'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            income = data_article['income'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids_article=input_ids_article,\n",
    "                attention_mask_article=attention_mask_article,\n",
    "                input_ids_essay=input_ids_essay,\n",
    "                attention_mask_essay=attention_mask_essay,\n",
    "                gender=gender,\n",
    "                education=education,\n",
    "                race=race,\n",
    "                age=age,\n",
    "                income=income\n",
    "            )          \n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            self.optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "            self.lr_scheduler.step()\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        epoch_loss = tr_loss / len(self.train_loader_article)\n",
    "        print(f'The total loss: {epoch_loss}')\n",
    "\n",
    "    def fit(self, save_model=False):\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            print(f'Epoch: {epoch+1}')\n",
    "            self._training_step(epoch)\n",
    "\n",
    "            preds = self.evaluate(dataloader_article=self.dev_loader_article,\n",
    "                                  dataloader_essay=self.dev_loader_essay, load_model=False)\n",
    "\n",
    "            dev_label = pd.read_csv(self.dev_label_file, sep='\\t', header=None)\n",
    "            if self.task[0] == 'empathy':\n",
    "                true = dev_label.iloc[:, 0].tolist()\n",
    "            if self.task[0] == 'distress':\n",
    "                true = dev_label.iloc[:, 1].tolist()\n",
    "            pearson_r = pearsonr(true, preds)\n",
    "            print(f'Pearson r: {pearson_r}')\n",
    "            \n",
    "            if save_model and (pearson_r > self.best_pearson_r):\n",
    "                self.best_pearson_r = pearson_r   \n",
    "                torch.save(self.model.state_dict(), 'roberta-empathy.pth')\n",
    "                print(\"Saved the model in epoch \" + str(epoch+1))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def evaluate(self, dataloader_article, dataloader_essay, load_model=False):\n",
    "        if load_model:\n",
    "            self.model.load_state_dict(torch.load('roberta-empathy.pth'))\n",
    "    \n",
    "        pred = torch.empty((len(dataloader_article.dataset), 1), device=self.device) # len(self.dev_loader.dataset) --> # of samples\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            for (data_article, data_essay) in zip(dataloader_article, dataloader_essay):\n",
    "                input_ids_article = data_article['input_ids'].to(self.device, dtype=torch.long)\n",
    "                attention_mask_article = data_article['attention_mask'].to(self.device, dtype=torch.long)\n",
    "                input_ids_essay = data_essay['input_ids'].to(self.device, dtype=torch.long)\n",
    "                attention_mask_essay = data_essay['attention_mask'].to(self.device, dtype=torch.long)\n",
    "\n",
    "                #taken from article dataloader. can also be taken from essay dataloader\n",
    "                gender = data_article['gender'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                education = data_article['education'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                race = data_article['race'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                age = data_article['age'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                income = data_article['income'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids_article=input_ids_article,\n",
    "                    attention_mask_article=attention_mask_article,\n",
    "                    input_ids_essay=input_ids_essay,\n",
    "                    attention_mask_essay=attention_mask_essay,\n",
    "                    gender=gender,\n",
    "                    education=education,\n",
    "                    race=race,\n",
    "                    age=age,\n",
    "                    income=income\n",
    "                )\n",
    "\n",
    "                batch_size = outputs.shape[0]\n",
    "                pred[idx:idx+batch_size, :] = outputs\n",
    "                idx += batch_size\n",
    "            \n",
    "        return [float(k) for k in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80e65bb5-f377-4664-8720-b18276ee77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "lr = 1e-05\n",
    "checkpoint = 'roberta-base'\n",
    "task_list = ['empathy', 'distress']\n",
    "feature_list = ['article', 'demographic_essay']\n",
    "\n",
    "train_file = './data/PREPROCESSED-WS22-WS23-train.tsv'\n",
    "# train_file = './data/COMBINED-PREPROCESSED-PARAPHRASED-WS22-WS23-train.tsv'\n",
    "\n",
    "# WASSA 2022\n",
    "# dev_file = './data/PREPROCESSED-WS22-dev.tsv'\n",
    "# dev_label_file = './data/WASSA22/goldstandard_dev_2022.tsv'\n",
    "# test_file = './data/PREPROCESSED-WS22-test.tsv'\n",
    "\n",
    "# WASSA 2023\n",
    "dev_file = './data/PREPROCESSED-WS23-dev.tsv'\n",
    "dev_label_file = './data/WASSA23/goldstandard_dev.tsv'\n",
    "test_file = './data/PREPROCESSED-WS23-test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc69ec6b-9bda-44f5-90b4-d02754f65d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module_article = DataModule(\n",
    "    task=[task_list[0]],\n",
    "    checkpoint=checkpoint,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    feature_to_tokenise=[feature_list[0]]\n",
    ")\n",
    "\n",
    "data_module_essay = DataModule(\n",
    "    task=[task_list[0]],\n",
    "    checkpoint=checkpoint,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    feature_to_tokenise=[feature_list[1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "871e3d4f-df64-42e0-8061-ac862da355de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader_article = data_module_article.dataloader(file=train_file, send_label=True, shuffle=True)\n",
    "dev_loader_article = data_module_article.dataloader(file=dev_file, send_label=False, shuffle=False)\n",
    "\n",
    "train_loader_essay = data_module_essay.dataloader(file=train_file, send_label=True, shuffle=True)\n",
    "dev_loader_essay = data_module_essay.dataloader(file=dev_file, send_label=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dd069104-06d8-40b6-9138-b95bba95be5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_all_seeds(0)\n",
    "model = BiEncoder(checkpoint=checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    task=[task_list[0]],\n",
    "    model=model,\n",
    "    lr=lr,\n",
    "    n_epoch=10,\n",
    "    train_loader_article=train_loader_article,\n",
    "    train_loader_essay=train_loader_essay,\n",
    "    dev_loader_article=dev_loader_article,\n",
    "    dev_loader_essay=dev_loader_essay,\n",
    "    dev_label_file=dev_label_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df391c70-1f6e-4156-ad7a-a3ea6459b914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "The total loss: 5.759729770819346\n",
      "Pearson r: 0.275\n",
      "Epoch: 2\n",
      "The total loss: 3.0137373674999584\n",
      "Pearson r: 0.379\n",
      "Epoch: 3\n",
      "The total loss: 2.3595968879533538\n",
      "Pearson r: 0.554\n",
      "Epoch: 4\n",
      "The total loss: 1.7387050343282295\n",
      "Pearson r: 0.556\n",
      "Epoch: 5\n",
      "The total loss: 1.1739419195236582\n",
      "Pearson r: 0.536\n",
      "Epoch: 6\n",
      "The total loss: 0.8594366723502224\n",
      "Pearson r: 0.541\n",
      "Epoch: 7\n",
      "The total loss: 0.6777805758245063\n",
      "Pearson r: 0.555\n",
      "Epoch: 8\n",
      "The total loss: 0.5228156495162032\n",
      "Pearson r: 0.553\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    save_model=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414781be-b9cb-4b42-b7fb-66853d341f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_float('Learning rate', 1e-05, 1e-03, log=True)\n",
    "    num_warmup = trial.suggest_int('Warmup steps', 0, 100)\n",
    "    beta_1 = \n",
    "    beta_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca845320-c8ab-4d99-8969-c5da1b110dbf",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78d5d6d3-b64d-4ffa-80bb-e20306380e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader_article = data_module_article.dataloader(file=test_file, send_label=False, shuffle=False)\n",
    "test_loader_essay = data_module_essay.dataloader(file=test_file, send_label=False, shuffle=False)\n",
    "pred = trainer.evaluate(dataloader_article=test_loader_article, dataloader_essay=test_loader_essay, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd902148-f104-4813-85bf-e0f52010023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'emp': pred, 'dis': pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80e697ba-1bf9-4787-9202-c15289b714e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emp</th>\n",
       "      <th>dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.348185</td>\n",
       "      <td>4.348185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.576570</td>\n",
       "      <td>4.576570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.480320</td>\n",
       "      <td>5.480320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.093744</td>\n",
       "      <td>4.093744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.806672</td>\n",
       "      <td>4.806672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3.575655</td>\n",
       "      <td>3.575655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.516463</td>\n",
       "      <td>5.516463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5.515535</td>\n",
       "      <td>5.515535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.277962</td>\n",
       "      <td>5.277962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4.690917</td>\n",
       "      <td>4.690917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emp       dis\n",
       "0   4.348185  4.348185\n",
       "1   4.576570  4.576570\n",
       "2   5.480320  5.480320\n",
       "3   4.093744  4.093744\n",
       "4   4.806672  4.806672\n",
       "..       ...       ...\n",
       "95  3.575655  3.575655\n",
       "96  5.516463  5.516463\n",
       "97  5.515535  5.515535\n",
       "98  5.277962  5.277962\n",
       "99  4.690917  4.690917\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eda740e-fd94-4f54-a9be-01774c5abc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('./tmp/predictions_EMP.tsv', sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174786c-ca4a-45ec-94ff-589272f892f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
