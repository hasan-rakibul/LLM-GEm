{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d78693-aff6-4484-9689-b92b91844b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_scheduler\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd96ca12-af79-4a12-8f47-218e424f8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"./text-empathy/\")\n",
    "from evaluation import pearsonr\n",
    "from utils.plot import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623f0a4d-9e54-42e5-bfe3-4c80f728a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['empathy', 'distress']\n",
    "\n",
    "checkpoint = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "train_file = \"./data/PREPROCESSED-essay-train.csv\"\n",
    "dev_file = \"./data/PREPROCESSED-essay-dev.csv\"\n",
    "train_dev_file = \"./data/PREPROCESSED-essay-train-dev.csv\"\n",
    "test_file = \"./data/PREPROCESSED-test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba39ac-c3c1-47be-98ae-c2752c8ff953",
   "metadata": {},
   "source": [
    "# Multi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f263859-7a93-4520-be66-8e0ed3424847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, task, feature_to_tokenise, train, batch_size, shuffle):\n",
    "\n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_to_tokenise], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[[feature_to_tokenise, task]]\n",
    "    else:\n",
    "        chosen_data = input_data[[feature_to_tokenise]]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "    \n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = [feature_to_tokenise])\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task, \"labels\") # as huggingface requires\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "           \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451d2b22-d74f-4be3-89c4-76f04b5f5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(task, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    trainloader, devloader = [], []\n",
    "    for feature in features:\n",
    "        trainloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=train_file, task=task, feature_to_tokenise=feature, train=True, batch_size=batch_size, shuffle=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # devloader in train mode to pass labels\n",
    "        devloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=dev_file, task=task, feature_to_tokenise=feature, train=True, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return trainloader, devloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b1bb74-49bf-4a64-ba41-3c3c3acecaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(task, features, batch_size, mode):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    testloader = []\n",
    "    for feature in features:\n",
    "        testloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=test_file, task=task, feature_to_tokenise=feature, train=False, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59655cfe-06dd-49b4-8a72-00ed65ab5e3d",
   "metadata": {},
   "source": [
    "## Tri-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5073dd-f797-439f-9219-03dda9ad38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['demographic', 'essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e104c968-3e79-4f42-bc16-1719396a79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(TriEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(2304, 768) #768+768+768 = 2304 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        self.fc4 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_f1=None,\n",
    "        attention_mask_f1=None,\n",
    "        input_ids_f2=None,\n",
    "        attention_mask_f2=None,\n",
    "        input_ids_f3=None,\n",
    "        attention_mask_f3=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs_f1 = self.transformer(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        outputs_f3 = self.transformer(\n",
    "            input_ids = input_ids_f3,\n",
    "            attention_mask = attention_mask_f3,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "        outputs_f3_last_state = outputs_f3[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "        outputs_f3_cls = outputs_f3_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls, outputs_f3_cls), dim=1) # shape: (batch_size, 768*3)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = (self.fc3(X))\n",
    "        logits = self.fc4(X)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac9b5da-2f6e-4d1f-93e7-9caff07f470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = TriEncoderTransformer(n_freeze=0)\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(task=task, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader[0])\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2, batch_f3) in zip(trainloader[0], trainloader[1], trainloader[2]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "                'attention_mask_f3': batch_f3['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader[0])) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2, batch_f3) in zip(devloader[0], devloader[1], devloader[2]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "                'attention_mask_f3': batch_f3['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device)\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            y_true.extend((batch_f1['labels'].tolist())) #batch_f2 labels should be the same\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader[0]))\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "    \n",
    "    # train-test finished\n",
    "    # torch.save(model.state_dict(), \"model.pth\")\n",
    "    # print(\"Saved the model as model.pth\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5e7dbf-b6ee-469f-867d-7452d888043f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.009\n",
      "pearson_r: -0.161\n",
      "pearson_r: 0.427\n",
      "pearson_r: 0.474\n",
      "pearson_r: 0.552\n",
      "pearson_r: 0.59\n",
      "pearson_r: 0.611\n",
      "pearson_r: 0.571\n",
      "pearson_r: 0.584\n",
      "pearson_r: 0.595\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(task, lr, batch_size, n_epochs, seed)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson_r:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pearsonr(y_true, y_pred))\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# train-test finished\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# torch.save(model.state_dict(), \"model.pth\")\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# print(\"Saved the model as model.pth\")\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mplot\u001b[49m(\n\u001b[1;32m     78\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     79\u001b[0m     y\u001b[38;5;241m=\u001b[39mtrain_loss,\n\u001b[1;32m     80\u001b[0m     y2\u001b[38;5;241m=\u001b[39mval_loss,\n\u001b[1;32m     81\u001b[0m     xlabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     82\u001b[0m     ylabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     legend\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     84\u001b[0m     save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     86\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "train(n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea23dab2-ff1b-4ef9-af0e-170106a0651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(task=tasks[0], batch_size=8):\n",
    "    \n",
    "    model = TriEncoderTransformer()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    # load the trained parameters\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    \n",
    "    testloader = get_test_data(task=task, features=features, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    for (batch_f1, batch_f2, batch_f3) in zip(testloader[0], testloader[1], testloader[2]):\n",
    "        batch = {\n",
    "            'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "            'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "            'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "            'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "            'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "            'attention_mask_f3': batch_f3['attention_mask'].to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "        y_pred.extend(batch_pred)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34ce7b0f-eca8-432a-93a5-d283989e1b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = test(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d5e6b-55ed-42f1-aee7-c14625b0202e",
   "metadata": {},
   "source": [
    "## Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1d6a54f-2f10-40cc-9099-119fe64c6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['demographic_essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2affe2e-f389-4d0c-996a-782b410da1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(BiEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer_f1 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.transformer_f2 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(1536, 768) #768+768 = 1536 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        self.fc4 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer_f1.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.transformer_f2.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer_f1.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                for layer in self.transformer_f2.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids_f1=None, attention_mask_f1=None, input_ids_f2=None, attention_mask_f2=None, labels=None):\n",
    "        outputs_f1 = self.transformer_f1(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer_f2(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls), dim=1) # shape: (batch_size, 1536)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        logits = self.fc4(X)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            # hidden_states=outputs.hidden_states,\n",
    "            # attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2debfb14-e1d6-4984-86ef-cc74781ff571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = BiEncoderTransformer()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(task=task, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader[0])\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2) in zip(trainloader[0], trainloader[1]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader[0])) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2) in zip(devloader[0], devloader[1]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device)\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            y_true.extend((batch_f1['labels'].tolist())) #batch_f2 labels should be the same\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader[0]))\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "    \n",
    "    # train-test finished\n",
    "    # torch.save(model.state_dict(), \"model.pth\")\n",
    "    # print(\"Saved the model as model.pth\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f0dba19-dc03-42a2-8243-b3e90c89b1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.175\n",
      "pearson_r: 0.485\n",
      "pearson_r: 0.476\n",
      "pearson_r: 0.55\n",
      "pearson_r: 0.549\n",
      "pearson_r: 0.586\n",
      "pearson_r: 0.615\n",
      "pearson_r: 0.61\n",
      "pearson_r: 0.602\n",
      "pearson_r: 0.599\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=10, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5d5a1-2ab5-47ec-877a-f931fde5c110",
   "metadata": {},
   "source": [
    "# Multi encoder MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27d4e111-cd3b-4eb4-8cf0-3522bb027b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, tasks, feature_to_tokenise, train, batch_size, shuffle):\n",
    "\n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_to_tokenise], truncation=True)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[[feature_to_tokenise] + tasks]\n",
    "    else:\n",
    "        chosen_data = input_data[[feature_to_tokenise]]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = [feature_to_tokenise])\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(tasks[0], \"labels_1\") # more meaningful\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(tasks[1], \"labels_2\")\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "           \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fea1566-713d-459c-bda2-1b4bc77fbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tasks, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    trainloader, devloader = [], []\n",
    "    for feature in features:\n",
    "        trainloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=train_file, tasks=tasks, feature_to_tokenise=feature, train=True, batch_size=batch_size, shuffle=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # devloader in train mode to pass labels\n",
    "        devloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=dev_file, tasks=tasks, feature_to_tokenise=feature, train=True, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return trainloader, devloader\n",
    "\n",
    "def get_test_data(tasks, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    testloader = []\n",
    "    for feature in features:\n",
    "        testloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=test_file, tasks=tasks, feature_to_tokenise=feature, train=False, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b5c5a-c841-410a-a7e3-2f0b4a66e708",
   "metadata": {},
   "source": [
    "## Tri-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91cc0d92-cc16-4703-804f-b5ab64239702",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['demographic', 'essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73625c01-0c9a-4095-b229-33783424241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriEncoderMTL(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(TriEncoderMTL, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(768*3, 768) #768*3 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        \n",
    "        self.fc4_1 = nn.Linear(256, 1) # regression problem\n",
    "        self.fc4_2 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_f1=None,\n",
    "        attention_mask_f1=None,\n",
    "        input_ids_f2=None,\n",
    "        attention_mask_f2=None,\n",
    "        input_ids_f3=None,\n",
    "        attention_mask_f3=None,\n",
    "        labels_1=None,\n",
    "        labels_2=None\n",
    "    ):\n",
    "        outputs_f1 = self.transformer(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        outputs_f3 = self.transformer(\n",
    "            input_ids = input_ids_f3,\n",
    "            attention_mask = attention_mask_f3,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "        outputs_f3_last_state = outputs_f3[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "        outputs_f3_cls = outputs_f3_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls, outputs_f3_cls), dim=1) # shape: (batch_size, 1536)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        \n",
    "        logits_1 = self.fc4_1(X)\n",
    "        logits_2 = self.fc4_2(X)\n",
    "        \n",
    "        loss_1 = None\n",
    "        if labels_1 is not None:\n",
    "            loss_1 = F.mse_loss(logits_1.view(-1), labels_1.view(-1))\n",
    "\n",
    "        loss_2 = None\n",
    "        if labels_2 is not None:\n",
    "            loss_2 = F.mse_loss(logits_2.view(-1), labels_2.view(-1))\n",
    "        \n",
    "        return (\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_1,\n",
    "                logits=logits_1\n",
    "            ),\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_2,\n",
    "                logits=logits_2\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a01e0b1-06a9-4bfb-b958-1e5956752bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = TriEncoderMTL()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(tasks=tasks, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader[0])\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    wgt_1=0.7\n",
    "    wgt_2=0.3\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2, batch_f3) in zip(trainloader[0], trainloader[1], trainloader[2]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "                'attention_mask_f3': batch_f3['attention_mask'].to(device),\n",
    "                'labels_1': batch_f1['labels_1'].to(device), #batch_f2 labels should be the same\n",
    "                'labels_2': batch_f1['labels_2'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            (outputs_1, outputs_2) = model(**batch)\n",
    "            loss_1 = outputs_1.loss\n",
    "            loss_2 = outputs_2.loss\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader[0])) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2, batch_f3) in zip(devloader[0], devloader[1], devloader[2]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "                'attention_mask_f3': batch_f3['attention_mask'].to(device),\n",
    "                'labels_1': batch_f1['labels_1'].to(device), #batch_f2 labels should be the same\n",
    "                'labels_2': batch_f1['labels_2'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                (outputs_1, outputs_2) = model(**batch)\n",
    "\n",
    "            batch_pred_1 = [item for sublist in outputs_1.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred_1)\n",
    "\n",
    "            y_true.extend((batch_f1['labels_1'].tolist())) #batch_f2 labels should be the same\n",
    "\n",
    "            loss_1 = outputs_1.loss.item()\n",
    "            loss_2 = outputs_2.loss.item()\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "\n",
    "            total_loss += loss\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader[0]))\n",
    "        \n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    # model_save_as = \"model-biencoder-mtl.pth\"\n",
    "    # torch.save(model.state_dict(), model_save_as)\n",
    "    # print(f\"Saved the model as {model_save_as}\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92637527-3438-4538-97d8-c7ed7ea88072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.078\n",
      "pearson_r: 0.394\n",
      "pearson_r: 0.246\n",
      "pearson_r: 0.209\n",
      "pearson_r: 0.472\n",
      "pearson_r: 0.498\n",
      "pearson_r: 0.414\n",
      "pearson_r: 0.403\n",
      "pearson_r: 0.425\n",
      "pearson_r: 0.446\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lr=1e-5, batch_size=8, n_epochs=10, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73f2cf70-8108-4624-bba9-8ad720a8ae06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.141\n",
      "pearson_r: 0.305\n",
      "pearson_r: 0.329\n",
      "pearson_r: -0.165\n",
      "pearson_r: 0.26\n",
      "pearson_r: 0.462\n",
      "pearson_r: 0.499\n",
      "pearson_r: 0.489\n",
      "pearson_r: 0.509\n",
      "pearson_r: 0.509\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lr=1e-5, batch_size=8, n_epochs=10, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8ba67-10ae-4d31-aa58-c2e571b0c1f1",
   "metadata": {},
   "source": [
    "## Bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36327c44-79c9-426a-bf49-720910fd210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['demographic_essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "256b6524-9ef8-440d-9a32-8f8803166c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderMTL(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(BiEncoderMTL, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(1536, 768) #768+768 = 1536 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        \n",
    "        self.fc4_1 = nn.Linear(256, 1) # regression problem\n",
    "        self.fc4_2 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_f1=None,\n",
    "        attention_mask_f1=None,\n",
    "        input_ids_f2=None,\n",
    "        attention_mask_f2=None,\n",
    "        labels_1=None,\n",
    "        labels_2=None\n",
    "    ):\n",
    "        outputs_f1 = self.transformer(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls), dim=1) # shape: (batch_size, 1536)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        \n",
    "        logits_1 = self.fc4_1(X)\n",
    "        logits_2 = self.fc4_2(X)\n",
    "        \n",
    "        loss_1 = None\n",
    "        if labels_1 is not None:\n",
    "            loss_1 = F.mse_loss(logits_1.view(-1), labels_1.view(-1))\n",
    "\n",
    "        loss_2 = None\n",
    "        if labels_2 is not None:\n",
    "            loss_2 = F.mse_loss(logits_2.view(-1), labels_2.view(-1))\n",
    "        \n",
    "        return (\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_1,\n",
    "                logits=logits_1\n",
    "            ),\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_2,\n",
    "                logits=logits_2\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e1fb28b-edc0-4a29-a3a0-138386504c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = BiEncoderMTL()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(tasks=tasks, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader[0])\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    wgt_1=1\n",
    "    wgt_2=1\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2) in zip(trainloader[0], trainloader[1]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels_1': batch_f1['labels_1'].to(device), #batch_f2 labels should be the same\n",
    "                'labels_2': batch_f1['labels_2'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            (outputs_1, outputs_2) = model(**batch)\n",
    "            loss_1 = outputs_1.loss\n",
    "            loss_2 = outputs_2.loss\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader[0])) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2) in zip(devloader[0], devloader[1]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels_1': batch_f1['labels_1'].to(device), #batch_f2 labels should be the same\n",
    "                'labels_2': batch_f1['labels_2'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                (outputs_1, outputs_2) = model(**batch)\n",
    "\n",
    "            batch_pred_1 = [item for sublist in outputs_1.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred_1)\n",
    "\n",
    "            y_true.extend((batch_f1['labels_1'].tolist())) #batch_f2 labels should be the same\n",
    "\n",
    "            loss_1 = outputs_1.loss.item()\n",
    "            loss_2 = outputs_2.loss.item()\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "\n",
    "            total_loss += loss\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader[0]))\n",
    "        \n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    # model_save_as = \"model-biencoder-mtl.pth\"\n",
    "    # torch.save(model.state_dict(), model_save_as)\n",
    "    # print(f\"Saved the model as {model_save_as}\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62344963-a18b-427d-acbe-d91bd0e1e43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.042\n",
      "pearson_r: 0.009\n",
      "pearson_r: -0.09\n",
      "pearson_r: 0.438\n",
      "pearson_r: 0.512\n",
      "pearson_r: 0.592\n",
      "pearson_r: 0.617\n",
      "pearson_r: 0.602\n",
      "pearson_r: 0.606\n",
      "pearson_r: 0.619\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lr=1e-5, batch_size=8, n_epochs=10, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c34a68ef-39ed-4520-9e36-aa1c46e94839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(batch_size=8):\n",
    "    \n",
    "    model = BiEncoderMTL()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    # load the trained parameters\n",
    "    model.load_state_dict(torch.load(\"model-biencoder-mtl.pth\"))\n",
    "    \n",
    "    testloader = get_test_data(tasks=tasks, features=features, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred_1, y_pred_2 = [], []\n",
    "    for (batch_f1, batch_f2) in zip(testloader[0], testloader[1]):\n",
    "        batch = {\n",
    "            'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "            'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "            'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "            'attention_mask_f2': batch_f2['attention_mask'].to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            (outputs_1, outputs_2) = model(**batch)\n",
    "\n",
    "        batch_pred_1 = [item for sublist in outputs_1.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "        y_pred_1.extend(batch_pred_1)\n",
    "        batch_pred_2 = [item for sublist in outputs_2.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "        y_pred_2.extend(batch_pred_2)\n",
    "\n",
    "    # for submission to CodaLab\n",
    "    y_pred = pd.DataFrame({\n",
    "        tasks[0]: y_pred_1,\n",
    "        tasks[1]: y_pred_2\n",
    "    })\n",
    "    y_pred.to_csv(\"./tmp/predictions_EMP.tsv\", sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3156124b-752e-4a85-8353-b2e3698afe2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819166a-84fa-49a7-b09e-2a1f12cae567",
   "metadata": {},
   "source": [
    "# Single encoder MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0990b1bb-2e93-4d12-bc68-21e2e579be60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, tasks, feature_to_tokenise, train, batch_size, shuffle):\n",
    "    feature_1 = feature_to_tokenise[0]\n",
    "    feature_2 = feature_to_tokenise[1]\n",
    "    \n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_1], sentence[feature_2], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[feature_to_tokenise + tasks] # + means list concat\n",
    "    else:\n",
    "        chosen_data = input_data[feature_to_tokenise]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = feature_to_tokenise)\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(tasks[0], \"labels_1\") # more meaningful\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(tasks[1], \"labels_2\")\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "           \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "24bb2661-c129-447c-81f6-632f2d63bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tasks, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    trainloader = load_tokenised_data(\n",
    "        filename=train_file, tasks=tasks, feature_to_tokenise=features, train=True, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "        # devloader in train mode to pass labels\n",
    "    devloader = load_tokenised_data(\n",
    "        filename=dev_file, tasks=tasks, feature_to_tokenise=features, train=True, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return trainloader, devloader\n",
    "\n",
    "def get_test_data(tasks, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    testloader = load_tokenised_data(\n",
    "        filename=test_file, tasks=tasks, feature_to_tokenise=features, train=False, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e33298f7-bf9a-4928-8502-0cc662ce2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleEncoderMTL(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(SingleEncoderMTL, self).__init__()\n",
    "        \n",
    "        self.transformer_last_hidden_size = 768\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "                num_labels=self.transformer_last_hidden_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(self.transformer_last_hidden_size, 256)\n",
    "        # self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3_1 = nn.Linear(256, 1) # regression problem\n",
    "        self.fc3_2 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.deberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.deberta.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels_1=None,\n",
    "        labels_2=None\n",
    "    ):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "\n",
    "        # print(outputs.keys())\n",
    "        # will return ['logits', 'hidden_states', 'attentions']\n",
    "        \n",
    "        X = self.dropout(outputs.logits)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        # X = F.tanh(self.fc2(X))\n",
    "        \n",
    "        logits_1 = self.fc3_1(X)\n",
    "        logits_2 = self.fc3_2(X)\n",
    "        \n",
    "        loss_1 = None\n",
    "        if labels_1 is not None:\n",
    "            loss_1 = F.mse_loss(logits_1.view(-1), labels_1.view(-1))\n",
    "\n",
    "        loss_2 = None\n",
    "        if labels_2 is not None:\n",
    "            loss_2 = F.mse_loss(logits_2.view(-1), labels_2.view(-1))\n",
    "        \n",
    "        return (\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_1,\n",
    "                logits=logits_1\n",
    "            ),\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_2,\n",
    "                logits=logits_2\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2a87e4bc-57bf-4ab8-b916-21229b493915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = SingleEncoderMTL()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(tasks=tasks, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    wgt_1=1\n",
    "    wgt_2=1\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            opt.zero_grad()\n",
    "            (outputs_1, outputs_2) = model(**batch)\n",
    "            loss_1 = outputs_1.loss\n",
    "            loss_2 = outputs_2.loss\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader)) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch in devloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                (outputs_1, outputs_2) = model(**batch)\n",
    "\n",
    "            batch_pred_1 = [item for sublist in outputs_1.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred_1)\n",
    "\n",
    "            y_true.extend((batch['labels_1'].tolist())) #batch_f2 labels should be the same\n",
    "\n",
    "            loss_1 = outputs_1.loss.item()\n",
    "            loss_2 = outputs_2.loss.item()\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "\n",
    "            total_loss += loss\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader))\n",
    "        \n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    # model_save_as = \"model-biencoder-mtl.pth\"\n",
    "    # torch.save(model.state_dict(), model_save_as)\n",
    "    # print(f\"Saved the model as {model_save_as}\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "00cd9433-8c47-41c6-a636-24448af99db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.141\n",
      "pearson_r: 0.406\n",
      "pearson_r: 0.522\n",
      "pearson_r: 0.627\n",
      "pearson_r: 0.603\n",
      "pearson_r: 0.664\n",
      "pearson_r: 0.677\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[136], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(task, lr, batch_size, n_epochs, seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 47\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)) \u001b[38;5;66;03m# len(trainloader) == num_batches\u001b[39;00m\n\u001b[1;32m     50\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=15, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d019e5-0361-4b43-a6e2-7a45c18a7b65",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d2adecf5-6ebf-4b2b-8ca9-5820ec461360",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = dev_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d25e607c-d7e9-4cf6-a9a0-e9d7da121fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_1 = pd.read_csv(\"./data/WASSA23/goldstandard_dev.tsv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6562dfc2-b47a-47c8-88a8-af54bc9fece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = pd.read_csv(\"./tmp/predictions_EMP.tsv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "caf34bbf-c558-437a-a646-dfdbc77fb261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(true_1.iloc[:, 0], pred_1.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa154a-d5b7-46c4-a85b-933f2cc38ded",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1be1e38-d79d-4f37-bc00-6bf3ce2983c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(train_file, header=0, index_col=0)\n",
    "pearsonr(data['empathy'], data['distress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d410c466-ce0b-45e5-83f2-7745af9e109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.594"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(test_file, header=0, index_col=0)\n",
    "pearsonr(data['empathy'], data['distress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3317864c-a416-4603-b8bc-d9119a0c8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.read_csv(\"./tmp/predictions_EMP.tsv\", sep='\\t', header=None, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "30ce2323-c15e-429d-b060-2b49c32807a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.906"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(y_pred.iloc[:,0], y_pred.iloc[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f296d90-1622-49a2-b27e-98923fdb1d0a",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd9005df-3ba4-414a-8d89-533e1a466ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.201\n",
      "pearson_r: 0.568\n",
      "pearson_r: 0.625\n",
      "pearson_r: 0.716\n",
      "pearson_r: 0.733\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.2\n",
      "pearson_r: 0.575\n",
      "pearson_r: 0.609\n",
      "pearson_r: 0.682\n",
      "pearson_r: 0.714\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.23\n",
      "pearson_r: 0.499\n",
      "pearson_r: 0.602\n",
      "pearson_r: 0.696\n",
      "pearson_r: 0.711\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.271\n",
      "pearson_r: 0.447\n",
      "pearson_r: 0.604\n",
      "pearson_r: 0.693\n",
      "pearson_r: 0.716\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.261\n",
      "pearson_r: 0.316\n",
      "pearson_r: 0.441\n",
      "pearson_r: 0.525\n",
      "pearson_r: 0.539\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.242\n",
      "pearson_r: 0.216\n",
      "pearson_r: 0.436\n",
      "pearson_r: 0.515\n",
      "pearson_r: 0.518\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.166\n",
      "pearson_r: 0.257\n",
      "pearson_r: 0.457\n",
      "pearson_r: 0.495\n",
      "pearson_r: 0.499\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.182\n",
      "pearson_r: 0.318\n",
      "pearson_r: 0.439\n",
      "pearson_r: 0.507\n",
      "pearson_r: 0.516\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.14\n",
      "pearson_r: 0.294\n",
      "pearson_r: 0.425\n",
      "pearson_r: 0.503\n",
      "pearson_r: 0.516\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.087\n",
      "pearson_r: 0.382\n",
      "pearson_r: 0.482\n",
      "pearson_r: 0.502\n",
      "pearson_r: 0.511\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.239\n",
      "pearson_r: 0.432\n",
      "pearson_r: 0.473\n",
      "pearson_r: 0.493\n",
      "pearson_r: 0.501\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.162\n",
      "pearson_r: 0.312\n",
      "pearson_r: 0.385\n",
      "pearson_r: 0.424\n",
      "pearson_r: 0.432\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7ce25289454bea81aa7fa3f4a96a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.139\n",
      "pearson_r: 0.166\n",
      "pearson_r: 0.189\n",
      "pearson_r: 0.2\n",
      "pearson_r: 0.205\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e7d1f-0c6b-48e0-8ba6-0f5b6d9526e0",
   "metadata": {},
   "source": [
    "## AutoModel vs AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2800df5-a062-4b38-a1e3-ac3ed77ab80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "75044741-ddb0-49e8-8c85-294dbec57f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTransformer(\n",
      "  (transformer): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (3): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "temp = CustomTransformer()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bebe2e60-ee9c-444a-aa67-d7fb344d2f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTransformerSequenceClassificaiton(\n",
      "  (transformer): DebertaV2ForSequenceClassification(\n",
      "    (deberta): DebertaV2Model(\n",
      "      (embeddings): DebertaV2Embeddings(\n",
      "        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "      (encoder): DebertaV2Encoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x DebertaV2Layer(\n",
      "            (attention): DebertaV2Attention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): DebertaV2SelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): DebertaV2Intermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DebertaV2Output(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (rel_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (pooler): ContextPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linearn): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "temp2 = CustomTransformerSequenceClassificaiton()\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9b9539de-c44c-40bc-a5be-8bb651aa2718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "CustomTransformer                                                 --\n",
       "DebertaV2Model: 1-1                                             --\n",
       "    DebertaV2Embeddings: 2-1                                   --\n",
       "        Embedding: 3-1                                        98,380,800\n",
       "        LayerNorm: 3-2                                        1,536\n",
       "        StableDropout: 3-3                                    --\n",
       "    DebertaV2Encoder: 2-2                                      --\n",
       "        ModuleList: 3-4                                       85,054,464\n",
       "        Embedding: 3-5                                        393,216\n",
       "        LayerNorm: 3-6                                        1,536\n",
       "Sequential: 1-2                                                 --\n",
       "    Linear: 2-3                                                590,592\n",
       "    Tanh: 2-4                                                  --\n",
       "    Linear: 2-5                                                590,592\n",
       "    Linear: 2-6                                                196,864\n",
       "    Linear: 2-7                                                257\n",
       "Dropout: 1-3                                                    --\n",
       "==========================================================================================\n",
       "Total params: 185,209,857\n",
       "Trainable params: 185,209,857\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d65a3053-5bbd-459a-a560-2548df8b83ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "CustomTransformerSequenceClassificaiton                                --\n",
       "DebertaV2ForSequenceClassification: 1-1                              --\n",
       "    DebertaV2Model: 2-1                                             --\n",
       "        DebertaV2Embeddings: 3-1                                   98,382,336\n",
       "        DebertaV2Encoder: 3-2                                      85,449,216\n",
       "    ContextPooler: 2-2                                              --\n",
       "        Linear: 3-3                                                590,592\n",
       "        StableDropout: 3-4                                         --\n",
       "    Linear: 2-3                                                     590,592\n",
       "    StableDropout: 2-4                                              --\n",
       "Dropout: 1-2                                                         --\n",
       "Linear: 1-3                                                          196,864\n",
       "Linear: 1-4                                                          257\n",
       "===============================================================================================\n",
       "Total params: 185,209,857\n",
       "Trainable params: 185,209,857\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2424bf-56e6-43df-83fe-e50e8b578fb5",
   "metadata": {},
   "source": [
    "## Earlier w/o *ForSequenceClassfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce85ff2e-a5a0-480e-9bea-42dcf4a22425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", num_labels=1):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        \n",
    "        self.num_labels=num_labels\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            ),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear1 = nn.Linear(768, 768)\n",
    "        self.linear2 = nn.Linear(768, 768)\n",
    "        self.linear3 = nn.Linear(768, 256)\n",
    "        self.linearn = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        \n",
    "        sequence_output = self.dropout(outputs[0]) #last_hidden_state\n",
    "        linear1_output = self.linear1(sequence_output[:,0,:].view(-1, 768)) #first token's embedding\n",
    "        linear2_output = self.linear2(F.tanh(linear1_output))\n",
    "        linear3_output = self.linear3(self.dropout(linear2_output))\n",
    "        logits = self.linearn(linear3_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6d3eb6-1029-41cb-b4f7-e639ce3ecf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(task=task, lr=1e-5, batch_size=8, n_epochs=3, n_freeze=0, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = CustomTransformer()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Freezing layers\n",
    "    if n_freeze:\n",
    "        # freeze the embedding layer when n_freeze = -1)\n",
    "        for param in model.transformer.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # freeze other layers as per n_freeze\n",
    "        if n_freeze != -1:\n",
    "            for layer in model.transformer.encoder.layer[:n_freeze]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    # print(\"Unfreezed layers:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainset = load_tokenised_data(\n",
    "        filename=train_file, task=task, tokenise_fn=tokenise, train=True\n",
    "    )\n",
    "           \n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # evaluation data loader in train mode to access labels\n",
    "    testset = load_tokenised_data(\n",
    "        filename=test_file, task=task, tokenise_fn=tokenise, train=True\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    training_steps = n_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(0, n_epochs):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch in testloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            y_true.extend((batch['labels'].tolist()))\n",
    "\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3e3295-42b9-4b56-b1b9-4d5aaf79ce41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.109\n",
      "pearson_r: 0.296\n",
      "pearson_r: 0.519\n",
      "pearson_r: 0.572\n",
      "pearson_r: 0.614\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.11\n",
      "pearson_r: 0.296\n",
      "pearson_r: 0.531\n",
      "pearson_r: 0.574\n",
      "pearson_r: 0.614\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.102\n",
      "pearson_r: 0.22\n",
      "pearson_r: 0.044\n",
      "pearson_r: 0.458\n",
      "pearson_r: 0.515\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.082\n",
      "pearson_r: 0.255\n",
      "pearson_r: 0.175\n",
      "pearson_r: 0.471\n",
      "pearson_r: 0.518\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.096\n",
      "pearson_r: 0.211\n",
      "pearson_r: 0.413\n",
      "pearson_r: 0.477\n",
      "pearson_r: 0.512\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.089\n",
      "pearson_r: 0.244\n",
      "pearson_r: 0.478\n",
      "pearson_r: 0.489\n",
      "pearson_r: 0.554\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.089\n",
      "pearson_r: 0.329\n",
      "pearson_r: 0.476\n",
      "pearson_r: 0.559\n",
      "pearson_r: 0.585\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.109\n",
      "pearson_r: 0.421\n",
      "pearson_r: 0.488\n",
      "pearson_r: 0.553\n",
      "pearson_r: 0.562\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.003\n",
      "pearson_r: 0.412\n",
      "pearson_r: 0.487\n",
      "pearson_r: 0.55\n",
      "pearson_r: 0.564\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.117\n",
      "pearson_r: 0.446\n",
      "pearson_r: 0.513\n",
      "pearson_r: 0.554\n",
      "pearson_r: 0.561\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.197\n",
      "pearson_r: 0.448\n",
      "pearson_r: 0.529\n",
      "pearson_r: 0.566\n",
      "pearson_r: 0.574\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.06\n",
      "pearson_r: 0.256\n",
      "pearson_r: 0.355\n",
      "pearson_r: 0.401\n",
      "pearson_r: 0.413\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f43f782100f45d6935ceece35239214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ba2907b3c148308ea336f30315a597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.155\n",
      "pearson_r: 0.177\n",
      "pearson_r: 0.203\n",
      "pearson_r: 0.207\n",
      "pearson_r: 0.215\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97529c70-6607-4447-8c2d-06bb551dffcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.243\n",
      "pearson_r: 0.449\n",
      "pearson_r: 0.51\n",
      "pearson_r: 0.553\n",
      "pearson_r: 0.57\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.246\n",
      "pearson_r: 0.454\n",
      "pearson_r: 0.51\n",
      "pearson_r: 0.558\n",
      "pearson_r: 0.575\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.405\n",
      "pearson_r: 0.48\n",
      "pearson_r: 0.505\n",
      "pearson_r: 0.57\n",
      "pearson_r: 0.585\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.398\n",
      "pearson_r: 0.455\n",
      "pearson_r: 0.551\n",
      "pearson_r: 0.612\n",
      "pearson_r: 0.625\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.258\n",
      "pearson_r: 0.415\n",
      "pearson_r: 0.531\n",
      "pearson_r: 0.589\n",
      "pearson_r: 0.592\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.261\n",
      "pearson_r: 0.446\n",
      "pearson_r: 0.494\n",
      "pearson_r: 0.541\n",
      "pearson_r: 0.582\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.212\n",
      "pearson_r: 0.423\n",
      "pearson_r: 0.45\n",
      "pearson_r: 0.493\n",
      "pearson_r: 0.524\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.307\n",
      "pearson_r: 0.445\n",
      "pearson_r: 0.477\n",
      "pearson_r: 0.533\n",
      "pearson_r: 0.554\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.22\n",
      "pearson_r: 0.41\n",
      "pearson_r: 0.459\n",
      "pearson_r: 0.475\n",
      "pearson_r: 0.485\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.195\n",
      "pearson_r: 0.369\n",
      "pearson_r: 0.436\n",
      "pearson_r: 0.473\n",
      "pearson_r: 0.483\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.062\n",
      "pearson_r: 0.36\n",
      "pearson_r: 0.458\n",
      "pearson_r: 0.494\n",
      "pearson_r: 0.504\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.127\n",
      "pearson_r: 0.268\n",
      "pearson_r: 0.341\n",
      "pearson_r: 0.383\n",
      "pearson_r: 0.394\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99d1b7e4004df69e6cf98e1cc802b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f77ca0147464c5ab3e4645ddf556fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.148\n",
      "pearson_r: 0.134\n",
      "pearson_r: 0.158\n",
      "pearson_r: 0.174\n",
      "pearson_r: 0.179\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Earlier without pooling effort\n",
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
