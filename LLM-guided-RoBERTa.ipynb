{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94915d97-0f63-4805-b4a3-66e94b3c5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "os.chdir(\"/g/data/jr19/rh2942/text-empathy/\")\n",
    "from evaluation import pearsonr\n",
    "from utils.utils import plot, get_device, set_all_seeds\n",
    "from utils.common import EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf579713-872e-41d6-8a8c-8f14a6e50498",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # due to huggingface warning\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ccd8e2-f398-46e3-8c29-b1a6c858c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(RobertaRegressor, self).__init__()\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=768)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(768, 512), nn.Tanh(), nn.Dropout(0.2)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512+5, 256), nn.Tanh(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        gender=None,\n",
    "        education=None,\n",
    "        race=None,\n",
    "        age=None,\n",
    "        income=None\n",
    "    ):\n",
    "\n",
    "        output = self.transformer(\n",
    "            input_ids= input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        output = self.fc1(output.logits)\n",
    "        output = torch.cat([output, gender, education, race, age, income], 1)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee314462-a714-4cea-80bb-82097bb625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, task, checkpoint, batch_size, feature_to_tokenise, seed):\n",
    "\n",
    "        self.task = task\n",
    "        self.checkpoint = checkpoint\n",
    "        self.batch_size = batch_size\n",
    "        self.tokeniser = AutoTokenizer.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokeniser)\n",
    "        self.feature_to_tokenise = feature_to_tokenise # to tokenise function\n",
    "        self.seed = seed\n",
    "\n",
    "        assert len(self.task) == 2, 'task must be a list with two elements'\n",
    "    \n",
    "    def _process_raw(self, path, send_label):\n",
    "        data = pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "        if send_label:\n",
    "            text = data[self.feature_to_tokenise + self.task]\n",
    "        else:\n",
    "            text = data[self.feature_to_tokenise]\n",
    "\n",
    "        demog = ['gender', 'education', 'race', 'age', 'income']\n",
    "        data_demog = data[demog]\n",
    "        scaler = MinMaxScaler()\n",
    "        data_demog = pd.DataFrame(\n",
    "            scaler.fit_transform(data_demog),\n",
    "            columns=demog\n",
    "        )\n",
    "        data = pd.concat([text, data_demog], axis=1) \n",
    "        return data\n",
    "\n",
    "    def _tokeniser_fn(self, sentence):\n",
    "        if len(self.feature_to_tokenise) == 1: # only one feature\n",
    "            return self.tokeniser(sentence[self.feature_to_tokenise[0]], truncation=True)\n",
    "        # otherwise tokenise a pair of sentence\n",
    "        return self.tokeniser(sentence[self.feature_to_tokenise[0]], sentence[self.feature_to_tokenise[1]], truncation=True)\n",
    "\n",
    "    def _process_input(self, file, send_label):\n",
    "        data = self._process_raw(path=file, send_label=send_label)\n",
    "        data = Dataset.from_pandas(data, preserve_index=False) # convert to huggingface dataset\n",
    "        data = data.map(self._tokeniser_fn, batched=True, remove_columns=self.feature_to_tokenise) # tokenise\n",
    "        data = data.with_format('torch')\n",
    "        return data\n",
    "\n",
    "    # taken from https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    def _seed_worker(self, worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)     \n",
    "\n",
    "    def dataloader(self, file, send_label, shuffle):\n",
    "        data = self._process_input(file=file, send_label=send_label)\n",
    "\n",
    "        # making sure the shuffling is reproducible\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.seed)\n",
    "        \n",
    "        return DataLoader(\n",
    "            data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=12,\n",
    "            worker_init_fn=self._seed_worker,\n",
    "            generator=g\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "095ee97b-ccaf-410a-bd26-5067564e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, task, checkpoint, lr, n_epochs, train_loader,\n",
    "                 dev_loader, dev_label_crowd, dev_label_gpt, device_id,\n",
    "                 anno_diff, first_fine_tune):\n",
    "        self.device = get_device(device_id)\n",
    "        self.task = task\n",
    "        self.checkpoint = checkpoint\n",
    "        self.first_fine_tune = first_fine_tune\n",
    "        \n",
    "        self.model = RobertaRegressor(checkpoint=self.checkpoint).to(self.device)\n",
    "    \n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.train_loader = train_loader\n",
    "        self.dev_loader = dev_loader\n",
    "        self.dev_label_crowd = dev_label_crowd\n",
    "        self.dev_label_gpt = dev_label_gpt\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimiser = torch.optim.AdamW(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-06,\n",
    "            weight_decay=0.1\n",
    "        )\n",
    "\n",
    "        n_training_step = self.n_epochs * len(self.train_loader)\n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=self.optimiser,\n",
    "            num_warmup_steps=0.06*n_training_step,\n",
    "            num_training_steps=n_training_step\n",
    "        )\n",
    "        \n",
    "        self.best_pearson_r = -1.0 # initiliasation\n",
    "        self.early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n",
    "        \n",
    "        self.anno_diff = anno_diff\n",
    "        \n",
    "        assert len(self.task) == 2, 'task must be a list with two elements'\n",
    "\n",
    "    def _label_fix(self, crowd, gpt):\n",
    "        condition = torch.abs(crowd.detach() - gpt.detach()) > self.anno_diff\n",
    "        crowd[condition] = gpt[condition]\n",
    "        return crowd\n",
    "\n",
    "    def _training_step(self):\n",
    "        tr_loss = 0.0\n",
    "        self.model.train()\n",
    "    \n",
    "        for data in self.train_loader:\n",
    "            input_ids = data['input_ids'].to(self.device, dtype=torch.long)\n",
    "            attention_mask = data['attention_mask'].to(self.device, dtype=torch.long)\n",
    "            \n",
    "            gpt = data[self.task[0]].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            targets = data[self.task[1]].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            \n",
    "            gender = data['gender'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            education = data['education'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            race = data['race'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            age = data['age'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            income = data['income'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                gender=gender,\n",
    "                education=education,\n",
    "                race=race,\n",
    "                age=age,\n",
    "                income=income\n",
    "            )\n",
    "\n",
    "            targets = self._label_fix(crowd=targets, gpt=gpt)\n",
    "            \n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "            self.optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "        epoch_loss = tr_loss / len(train_loader)\n",
    "        print(f'Train loss: {epoch_loss}')\n",
    "\n",
    "    def fit(self, save_model=False):\n",
    "        dev_label_anno = pd.read_csv(self.dev_label_gpt, sep='\\t')\n",
    "        true_gpt = dev_label_anno.loc[:, 'empathy'].tolist()\n",
    "        dev_label_crowd = pd.read_csv(self.dev_label_crowd, sep='\\t', header=None)\n",
    "        true = dev_label_crowd.iloc[:, 0].tolist()\n",
    "\n",
    "        # gpt-label on the second fine tune \n",
    "        true = self._label_fix(crowd=torch.tensor(true), gpt=torch.tensor(true_gpt))\n",
    "        true = true.tolist()\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            print(f'Epoch: {epoch+1}')\n",
    "            self._training_step()\n",
    "\n",
    "            preds = self.evaluate(dataloader=self.dev_loader, load_model=False)\n",
    "            \n",
    "            pearson_r = pearsonr(true, preds)\n",
    "            print(f'Pearson r: {pearson_r}')\n",
    "            \n",
    "            val_loss = self.loss_fn(torch.tensor(preds), torch.tensor(true))\n",
    "            print('Validation loss:', val_loss.item())\n",
    "            \n",
    "            if self.early_stopper.early_stop(val_loss):\n",
    "                break\n",
    "\n",
    "            if (pearson_r > self.best_pearson_r):\n",
    "                self.best_pearson_r = pearson_r\n",
    "\n",
    "                # save\n",
    "                if save_model:\n",
    "                    torch.save(self.model.state_dict(), self.checkpoint+'-gpt-finetuned.pth')\n",
    "                    print(\"Saved the second finetuned model in epoch \" + str(epoch+1))\n",
    "            \n",
    "            print(f'Best dev set Pearson r: {self.best_pearson_r}\\n')\n",
    "\n",
    "    def evaluate(self, dataloader, load_model=False):\n",
    "        if load_model:\n",
    "            self.model.load_state_dict(torch.load(self.checkpoint+'-gpt-finetuned.pth'))\n",
    "    \n",
    "        pred = torch.empty((len(dataloader.dataset), 1), device=self.device) # len(self.dev_loader.dataset) --> # of samples\n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            for data in dataloader:\n",
    "                input_ids = data['input_ids'].to(self.device, dtype=torch.long)\n",
    "                attention_mask = data['attention_mask'].to(self.device, dtype=torch.long)\n",
    "                gender = data['gender'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                education = data['education'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                race = data['race'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                age = data['age'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                income = data['income'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "        \n",
    "                outputs = self.model(input_ids, attention_mask, gender, education, race, age, income)\n",
    "\n",
    "                batch_size = outputs.shape[0]\n",
    "                pred[idx:idx+batch_size, :] = outputs\n",
    "                idx += batch_size\n",
    "            \n",
    "        return [float(k) for k in pred]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80e65bb5-f377-4664-8720-b18276ee77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'roberta-base'\n",
    "task = ['empathy', 'wrong_empathy']\n",
    "# feature_to_tokenise=['demographic_essay', 'article']\n",
    "# feature_to_tokenise=['demographic', 'essay']\n",
    "feature_to_tokenise=['demographic_essay']\n",
    "seed = 0\n",
    "\n",
    "################# COMBINED TRAIN FILE ##############\n",
    "# train_file = './data/WS22-WS23-train-gpt.tsv'\n",
    "train_file = './data/WS22-WS23-sep-from-aug-train-gpt.tsv'\n",
    "# train_file = './data/essay-train-ws22-ws23.tsv'\n",
    "# train_file = './data/PREPROCESSED-WS22-WS23-train.tsv'\n",
    "# train_file = './data/COMBINED-PREPROCESSED-PARAPHRASED-WS22-WS23-train.tsv'\n",
    "\n",
    "################# WASSA 2022 ####################\n",
    "# dev_file = './data/PREPROCESSED-WS22-dev.tsv'\n",
    "# dev_label_file = './data/WASSA22/goldstandard_dev_2022.tsv'\n",
    "# test_file = './data/PREPROCESSED-WS22-test.tsv'\n",
    "\n",
    "##### GPT annotation\n",
    "# train_file = './data/WS22-train-gpt.tsv'\n",
    "# dev_file = './data/WS22-dev-gpt.tsv'\n",
    "# dev_label_file = './data/WS22-dev-gpt.tsv'\n",
    "\n",
    "\n",
    "################# WASSA 2023 ####################\n",
    "# dev_file = './data/PREPROCESSED-WS23-dev.tsv'\n",
    "dev_label_crowd = './data/WASSA23/goldstandard_dev.tsv'\n",
    "test_file = './data/PREPROCESSED-WS23-test.tsv'\n",
    "\n",
    "##### GPT annotation\n",
    "# train_file = './data/WS23-train-gpt.tsv'\n",
    "dev_file = './data/WS23-dev-gpt.tsv'\n",
    "dev_label_gpt = './data/WS23-dev-gpt.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc69ec6b-9bda-44f5-90b4-d02754f65d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 13.341799072063331\n",
      "Pearson r: 0.209\n",
      "Validation loss: 2.5673940181732178\n",
      "Best dev set Pearson r: 0.209\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 2.7105073563980335\n",
      "Pearson r: 0.617\n",
      "Validation loss: 1.3849575519561768\n",
      "Best dev set Pearson r: 0.617\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 1.7792145237778172\n",
      "Pearson r: 0.658\n",
      "Validation loss: 1.192095398902893\n",
      "Best dev set Pearson r: 0.658\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 1.3710925178094344\n",
      "Pearson r: 0.709\n",
      "Validation loss: 1.099982738494873\n",
      "Best dev set Pearson r: 0.709\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 1.1807976664918842\n",
      "Pearson r: 0.699\n",
      "Validation loss: 1.0979441404342651\n",
      "Best dev set Pearson r: 0.709\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 0.9269254198580077\n",
      "Pearson r: 0.71\n",
      "Validation loss: 1.0793267488479614\n",
      "Best dev set Pearson r: 0.71\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 0.7482946957602645\n",
      "Pearson r: 0.713\n",
      "Validation loss: 1.0665911436080933\n",
      "Best dev set Pearson r: 0.713\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 0.6437775433966608\n",
      "Pearson r: 0.713\n",
      "Validation loss: 1.090736985206604\n",
      "Best dev set Pearson r: 0.713\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 0.5415739933649699\n",
      "Pearson r: 0.714\n",
      "Validation loss: 1.099827766418457\n",
      "Best dev set Pearson r: 0.714\n",
      "\n",
      "Epoch: 10\n",
      "Train loss: 0.5038516980229002\n",
      "Pearson r: 0.713\n",
      "Validation loss: 1.1114075183868408\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds(seed)\n",
    "\n",
    "data_module = DataModule(\n",
    "    task=task,\n",
    "    checkpoint=checkpoint,\n",
    "    batch_size=16,\n",
    "    feature_to_tokenise=feature_to_tokenise,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "train_loader = data_module.dataloader(file=train_file, send_label=True, shuffle=True)\n",
    "dev_loader = data_module.dataloader(file=dev_file, send_label=False, shuffle=False)\n",
    "test_loader = data_module.dataloader(file=test_file, send_label=False, shuffle=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    task=task,\n",
    "    checkpoint=checkpoint,\n",
    "    lr=1e-5,\n",
    "    n_epochs=10,\n",
    "    train_loader=train_loader,\n",
    "    dev_loader=dev_loader,\n",
    "    dev_label_gpt=dev_label_gpt,\n",
    "    dev_label_crowd=dev_label_crowd,\n",
    "    device_id=0,\n",
    "    anno_diff=1.5,\n",
    "    first_fine_tune=False\n",
    ")\n",
    "\n",
    "trainer.fit(save_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15c9c9-d937-4f8a-825f-7ef384dad0f3",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78d5d6d3-b64d-4ffa-80bb-e20306380e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emp</th>\n",
       "      <th>dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.738305</td>\n",
       "      <td>3.738305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.001295</td>\n",
       "      <td>6.001295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.443245</td>\n",
       "      <td>5.443245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.081530</td>\n",
       "      <td>2.081530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.864893</td>\n",
       "      <td>5.864893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.833966</td>\n",
       "      <td>5.833966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4.599444</td>\n",
       "      <td>4.599444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5.307490</td>\n",
       "      <td>5.307490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6.035558</td>\n",
       "      <td>6.035558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.891054</td>\n",
       "      <td>5.891054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emp       dis\n",
       "0   3.738305  3.738305\n",
       "1   6.001295  6.001295\n",
       "2   5.443245  5.443245\n",
       "3   2.081530  2.081530\n",
       "4   5.864893  5.864893\n",
       "..       ...       ...\n",
       "95  5.833966  5.833966\n",
       "96  4.599444  4.599444\n",
       "97  5.307490  5.307490\n",
       "98  6.035558  6.035558\n",
       "99  5.891054  5.891054\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = trainer.evaluate(dataloader=test_loader, load_model=True)\n",
    "pred_df = pd.DataFrame({'emp': pred, 'dis': pred}) # we're not predicting distress, just aligning with submission system\n",
    "pred_df.to_csv('./tmp/predictions_EMP.tsv', sep='\\t', index=None, header=None)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5253f-7947-42e1-9e3a-535b1e3d948c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
