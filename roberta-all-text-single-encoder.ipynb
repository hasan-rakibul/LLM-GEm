{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94915d97-0f63-4805-b4a3-66e94b3c5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModel,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "os.chdir(\"/g/data/jr19/rh2942/text-empathy/\")\n",
    "from evaluation import pearsonr\n",
    "from utils.utils import plot, get_device, set_all_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf579713-872e-41d6-8a8c-8f14a6e50498",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # due to huggingface warning\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee314462-a714-4cea-80bb-82097bb625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule():\n",
    "    def __init__(self, task, checkpoint, batch_size, max_len, feature_to_tokenise):\n",
    "        super(DataModule, self).__init__()\n",
    "        self.task = task\n",
    "        self.checkpoint = checkpoint\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.tokeniser = AutoTokenizer.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokeniser)\n",
    "        self.feature_to_tokenise = feature_to_tokenise # to tokenise function\n",
    "    \n",
    "    def _process_raw(self, path, send_label):\n",
    "        data = pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "        if send_label:\n",
    "            data = data[self.feature_to_tokenise + [self.task]]\n",
    "        else:\n",
    "            data = data[self.feature_to_tokenise]\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def _tokeniser_fn(self, sentence):\n",
    "        if len(self.feature_to_tokenise) == 1: # only one feature\n",
    "            return self.tokeniser(sentence[self.feature_to_tokenise[0]], truncation=True)\n",
    "        # otherwise tokenise a pair of sentence\n",
    "        return self.tokeniser(sentence[self.feature_to_tokenise[0]], sentence[self.feature_to_tokenise[1]], truncation=True)\n",
    "\n",
    "    def _process_input(self, file, send_label):\n",
    "        data = self._process_raw(path=file, send_label=send_label)\n",
    "        data = data.reset_index(drop=True)\n",
    "        data = Dataset.from_pandas(data, preserve_index=False) # convert to huggingface dataset\n",
    "        data = data.map(self._tokeniser_fn, batched=True, remove_columns=self.feature_to_tokenise) # tokenise\n",
    "        data = data.with_format('torch')\n",
    "        return data\n",
    "\n",
    "    def dataloader(self, file, send_label, shuffle):\n",
    "        data = self._process_input(file=file, send_label=send_label)\n",
    "        return DataLoader(\n",
    "            data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=24\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ccd8e2-f398-46e3-8c29-b1a6c858c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleEncoder(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(SingleEncoder, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(checkpoint)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        \n",
    "        self.classifier = nn.Linear(768, 512)\n",
    "        self.pre_final = nn.Linear(512, 256)\n",
    "        self.final = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "    ):\n",
    "\n",
    "        output = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        output = output[0]\n",
    "        output = output[:, 0]\n",
    "\n",
    "        output = self.pre_classifier(output)\n",
    "        \n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.classifier(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.pre_final(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.final(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095ee97b-ccaf-410a-bd26-5067564e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, task, model, lr):\n",
    "        self.device = get_device(0)\n",
    "        \n",
    "        self.task = task\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr = self.lr)\n",
    "        \n",
    "        self.best_pearson_r = -1.0 # initiliasation\n",
    "\n",
    "    def _training_step(self, epoch, train_loader):\n",
    "        tr_loss = 0.0\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            input_ids = data['input_ids'].to(self.device, dtype=torch.long)\n",
    "            attention_mask = data['attention_mask'].to(self.device, dtype=torch.long)\n",
    "            # token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "            targets = data[self.task].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                # token_type_ids=token_type_ids,\n",
    "            )          \n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "            if i % 50 == 0:\n",
    "                loss_step = tr_loss / (i+1)\n",
    "                print(f'Training loss per 50 steps: {loss_step}')\n",
    "    \n",
    "        epoch_loss = tr_loss / (i+1)\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'The total loss: {epoch_loss}')\n",
    "\n",
    "    def fit(self, n_epochs, train_loader, dev_loader, output_label, save_model=False):\n",
    "        set_all_seeds(0)\n",
    "        for epoch in range(n_epochs):\n",
    "            self._training_step(epoch, train_loader)\n",
    "\n",
    "            preds = self.evaluate(dataloader=dev_loader, load_model=False)\n",
    "            if self.task == 'empathy':\n",
    "                true = output_label.iloc[:, 0].tolist()\n",
    "            if self.task == 'distress':\n",
    "                true = output_label.iloc[:, 1].tolist()\n",
    "            pearson_r = pearsonr(true, preds)\n",
    "            print(f'Pearson r: {pearson_r}')\n",
    "            \n",
    "            if save_model and (pearson_r > self.best_pearson_r):\n",
    "                self.best_pearson_r = pearson_r   \n",
    "                torch.save(self.model.state_dict(), 'roberta-empathy.pth')\n",
    "                print(\"Saved the model in epoch \" + str(epoch+1))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def evaluate(self, dataloader, load_model=False):\n",
    "        if load_model:\n",
    "            self.model.load_state_dict(torch.load('roberta-empathy.pth'))\n",
    "    \n",
    "        pred = torch.empty((len(dataloader.dataset), 1), device=self.device) # len(self.dev_loader.dataset) --> # of samples\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            for data in dataloader:\n",
    "                input_ids = data['input_ids'].to(self.device, dtype=torch.long)\n",
    "                attention_mask = data['attention_mask'].to(self.device, dtype=torch.long)\n",
    "                # token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    # token_type_ids=token_type_ids,\n",
    "                )\n",
    "\n",
    "                batch_size = outputs.shape[0]\n",
    "                pred[idx:idx+batch_size, :] = outputs\n",
    "                idx += batch_size\n",
    "            \n",
    "        return [float(k) for k in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e65bb5-f377-4664-8720-b18276ee77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "lr = 1e-05\n",
    "checkpoint = 'roberta-base'\n",
    "task_list = ['empathy', 'distress']\n",
    "\n",
    "train_file = './data/PREPROCESSED-WS22-WS23-train.tsv'\n",
    "\n",
    "# WASSA 2022\n",
    "# dev_file = './data/PREPROCESSED-WS22-dev.tsv'\n",
    "# dev_label_file = './data/WASSA22/goldstandard_dev_2022.tsv'\n",
    "# test_file = './data/PREPROCESSED-WS22-test.tsv'\n",
    "\n",
    "# WASSA 2023\n",
    "dev_file = './data/PREPROCESSED-WS23-dev.tsv'\n",
    "dev_label_file = './data/WASSA23/goldstandard_dev.tsv'\n",
    "test_file = './data/PREPROCESSED-WS23-test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc69ec6b-9bda-44f5-90b4-d02754f65d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    task=task_list[0],\n",
    "    checkpoint=checkpoint,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    feature_to_tokenise=['demographic_essay', 'article']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871e3d4f-df64-42e0-8061-ac862da355de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = data_module.dataloader(file=train_file, send_label=True, shuffle=True)\n",
    "dev_loader = data_module.dataloader(file=dev_file, send_label=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d861b3-051d-4d8b-80fa-b8e2c3551b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_label = pd.read_csv(dev_label_file, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd069104-06d8-40b6-9138-b95bba95be5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SingleEncoder(checkpoint=checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    task=task_list[0],\n",
    "    model=model,\n",
    "    lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bbeb466-3f4c-4364-bd13-d0ceb0707bbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 50 steps: 10.632017135620117\n",
      "Training loss per 50 steps: 14.69898635265874\n",
      "Training loss per 50 steps: 11.864423293878536\n",
      "Training loss per 50 steps: 9.437687400161035\n",
      "Training loss per 50 steps: 8.015810783822738\n",
      "Training loss per 50 steps: 7.121970001919811\n",
      "Training loss per 50 steps: 6.587764896823718\n",
      "Epoch: 1\n",
      "The total loss: 6.328615145972281\n",
      "Pearson r: 0.197\n",
      "Training loss per 50 steps: 4.39067268371582\n",
      "Training loss per 50 steps: 3.508858456331141\n",
      "Training loss per 50 steps: 3.50756897696174\n",
      "Training loss per 50 steps: 3.581367394189961\n",
      "Training loss per 50 steps: 3.5898176075807258\n",
      "Training loss per 50 steps: 3.567614012030491\n",
      "Training loss per 50 steps: 3.5955927181877567\n",
      "Epoch: 2\n",
      "The total loss: 3.6091153928727815\n",
      "Pearson r: 0.225\n",
      "Training loss per 50 steps: 4.518770694732666\n",
      "Training loss per 50 steps: 3.5791295813579187\n",
      "Training loss per 50 steps: 3.3385148768377775\n",
      "Training loss per 50 steps: 3.3210338314637444\n",
      "Training loss per 50 steps: 3.2169814006013064\n",
      "Training loss per 50 steps: 3.188368447985782\n",
      "Training loss per 50 steps: 3.113751394210068\n",
      "Epoch: 3\n",
      "The total loss: 3.084641689423359\n",
      "Pearson r: 0.538\n",
      "Training loss per 50 steps: 1.3612014055252075\n",
      "Training loss per 50 steps: 2.534024375326493\n",
      "Training loss per 50 steps: 2.5857903396729194\n",
      "Training loss per 50 steps: 2.4540450786123214\n",
      "Training loss per 50 steps: 2.4518743121801916\n",
      "Training loss per 50 steps: 2.4181493423374527\n",
      "Training loss per 50 steps: 2.4071011654166288\n",
      "Epoch: 4\n",
      "The total loss: 2.4180038968722024\n",
      "Pearson r: 0.575\n",
      "Training loss per 50 steps: 1.4533518552780151\n",
      "Training loss per 50 steps: 1.8440434769088148\n",
      "Training loss per 50 steps: 1.7289720915331699\n",
      "Training loss per 50 steps: 1.7356598714724283\n",
      "Training loss per 50 steps: 1.7570131822901578\n",
      "Training loss per 50 steps: 1.802204695830782\n",
      "Training loss per 50 steps: 1.814752344475236\n",
      "Epoch: 5\n",
      "The total loss: 1.8397964757500274\n",
      "Pearson r: 0.57\n",
      "Training loss per 50 steps: 2.7683887481689453\n",
      "Training loss per 50 steps: 1.3823515802037483\n",
      "Training loss per 50 steps: 1.5353236316454293\n",
      "Training loss per 50 steps: 1.3846041496028962\n",
      "Training loss per 50 steps: 1.3382737302216725\n",
      "Training loss per 50 steps: 1.2870428354972863\n",
      "Training loss per 50 steps: 1.2660019979425443\n",
      "Epoch: 6\n",
      "The total loss: 1.2526991473454419\n",
      "Pearson r: 0.566\n",
      "Training loss per 50 steps: 0.2385970652103424\n",
      "Training loss per 50 steps: 0.7677038218460831\n",
      "Training loss per 50 steps: 0.7972466601888732\n",
      "Training loss per 50 steps: 0.749229799882071\n",
      "Training loss per 50 steps: 0.7848990480624028\n",
      "Training loss per 50 steps: 0.8206815097139651\n",
      "Training loss per 50 steps: 0.8405649188992589\n",
      "Epoch: 7\n",
      "The total loss: 0.8324576530718442\n",
      "Pearson r: 0.594\n",
      "Training loss per 50 steps: 0.15064480900764465\n",
      "Training loss per 50 steps: 0.5860999819694781\n",
      "Training loss per 50 steps: 0.5450063833210728\n",
      "Training loss per 50 steps: 0.5513795815556255\n",
      "Training loss per 50 steps: 0.5338045580507215\n",
      "Training loss per 50 steps: 0.5416636381639665\n",
      "Training loss per 50 steps: 0.5593330469663159\n",
      "Epoch: 8\n",
      "The total loss: 0.5545238725276608\n",
      "Pearson r: 0.557\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(n_epochs=8, train_loader=train_loader, dev_loader=dev_loader,\n",
    "            output_label=dev_label, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51acbdec-493c-4cd5-b03e-e976d6a72b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78d5d6d3-b64d-4ffa-80bb-e20306380e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader = data_module.dataloader(file=test_file, send_label=False, shuffle=False)\n",
    "pred = trainer.evaluate(dataloader=test_loader, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd902148-f104-4813-85bf-e0f52010023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'emp': pred, 'dis': pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80e697ba-1bf9-4787-9202-c15289b714e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emp</th>\n",
       "      <th>dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.743536</td>\n",
       "      <td>4.743536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.375017</td>\n",
       "      <td>5.375017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.558261</td>\n",
       "      <td>5.558261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.441684</td>\n",
       "      <td>4.441684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.513950</td>\n",
       "      <td>4.513950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.794048</td>\n",
       "      <td>5.794048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.106700</td>\n",
       "      <td>5.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.298420</td>\n",
       "      <td>3.298420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4.998303</td>\n",
       "      <td>4.998303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.430133</td>\n",
       "      <td>5.430133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emp       dis\n",
       "0   4.743536  4.743536\n",
       "1   5.375017  5.375017\n",
       "2   5.558261  5.558261\n",
       "3   4.441684  4.441684\n",
       "4   4.513950  4.513950\n",
       "..       ...       ...\n",
       "95  5.794048  5.794048\n",
       "96  5.106700  5.106700\n",
       "97  3.298420  3.298420\n",
       "98  4.998303  4.998303\n",
       "99  5.430133  5.430133\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1eda740e-fd94-4f54-a9be-01774c5abc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('./tmp/predictions_EMP.tsv', sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f195a-0ffe-4366-b3d0-835dc3318ef5",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034b5d0-22fb-4221-a0d1-0bee1058021f",
   "metadata": {},
   "source": [
    "## Combined guide and final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3edc3ddd-3ed6-4125-b567-1f33a6e08bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedRegression(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(GuidedRegression, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(checkpoint)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.pre_classifier_guide = nn.Linear(769, 768)\n",
    "        \n",
    "        self.classifier = nn.Linear(768, 512)\n",
    "        self.pre_final = nn.Linear(512, 256)\n",
    "        self.final = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "        guide=None\n",
    "    ):\n",
    "\n",
    "        output = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        output = output[0]\n",
    "        output = output[:, 0]\n",
    "\n",
    "        if guide is None:\n",
    "            output = self.pre_classifier(output)\n",
    "        else:\n",
    "            output = torch.cat([output, guide], 1)\n",
    "            output = self.pre_classifier_guide(output)\n",
    "        \n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.classifier(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.pre_final(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.final(output)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
