{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecc4c5c-638d-4e36-bf1b-f4884e25c1fc",
   "metadata": {},
   "source": [
    "# Numerical demographic as intermediate fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94915d97-0f63-4805-b4a3-66e94b3c5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModel,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "os.chdir(\"/g/data/jr19/rh2942/text-empathy/\")\n",
    "from evaluation import pearsonr\n",
    "from utils.utils import plot, get_device, set_all_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf579713-872e-41d6-8a8c-8f14a6e50498",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # due to huggingface warning\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ccd8e2-f398-46e3-8c29-b1a6c858c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(RobertaRegressor, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(checkpoint)\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(768, 512)\n",
    "        self.pre_final = nn.Linear(517, 256)\n",
    "        self.final = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "        gender=None,\n",
    "        education=None,\n",
    "        race=None,\n",
    "        age=None,\n",
    "        income=None\n",
    "    ):\n",
    "\n",
    "        output = self.transformer(\n",
    "            input_ids= input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids\n",
    "        )\n",
    "        output = output[0]\n",
    "        output = output[:, 0]\n",
    "        output = self.pre_classifier(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.classifier(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = torch.cat([output, gender, education, race, age, income], 1)\n",
    "        output = self.pre_final(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.final(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee314462-a714-4cea-80bb-82097bb625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule():\n",
    "    def __init__(self, task, checkpoint, batch_size, max_len, feature_to_tokenise):\n",
    "        super(DataModule, self).__init__()\n",
    "        self.task = task\n",
    "        self.checkpoint = checkpoint\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.tokeniser = AutoTokenizer.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokeniser)\n",
    "        self.feature_to_tokenise = feature_to_tokenise # to tokenise function\n",
    "    \n",
    "    def _process_raw(self, path, send_label):\n",
    "        data = pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "        if send_label:\n",
    "            text = data[self.feature_to_tokenise + self.task]\n",
    "        else:\n",
    "            text = data[self.feature_to_tokenise]\n",
    "\n",
    "        data_demog = data[demog]\n",
    "        scaler = MinMaxScaler()\n",
    "        data_demog = pd.DataFrame(\n",
    "            scaler.fit_transform(data_demog),\n",
    "            columns=demog\n",
    "        )\n",
    "        data = pd.concat([text, data_demog], axis=1) \n",
    "        return data\n",
    "\n",
    "    def _tokeniser_fn(self, sentence):\n",
    "        if len(self.feature_to_tokenise) == 1: # only one feature\n",
    "            return self.tokeniser(sentence[self.feature_to_tokenise[0]], truncation=True)\n",
    "        # otherwise tokenise a pair of sentence\n",
    "        return self.tokeniser(sentence[self.feature_to_tokenise[0]], sentence[self.feature_to_tokenise[1]], truncation=True)\n",
    "\n",
    "    def _process_input(self, file, send_label):\n",
    "        data = self._process_raw(path=file, send_label=send_label)\n",
    "        data = data.reset_index(drop=True)\n",
    "        data = Dataset.from_pandas(data, preserve_index=False) # convert to huggingface dataset\n",
    "        data = data.map(self._tokeniser_fn, batched=True, remove_columns=self.feature_to_tokenise) # tokenise\n",
    "        data = data.with_format('torch')\n",
    "        return data\n",
    "\n",
    "    def dataloader(self, file, send_label, shuffle):\n",
    "        data = self._process_input(file=file, send_label=send_label)\n",
    "        return DataLoader(\n",
    "            data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=24\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "095ee97b-ccaf-410a-bd26-5067564e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, task, model, lr):\n",
    "        self.device = get_device(0)\n",
    "        self.task = task\n",
    "        self.model = model.to(self.device)\n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr = self.lr)\n",
    "        self.best_pearson_r = -1.0 # initiliasation\n",
    "\n",
    "    def _training_step(self, epoch, train_loader):\n",
    "        tr_loss = 0.0\n",
    "        self.model.train()\n",
    "    \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            input_ids = data['input_ids'].to(self.device, dtype=torch.long)\n",
    "            attention_mask = data['attention_mask'].to(self.device, dtype=torch.long)\n",
    "            # token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "            targets = data[self.task].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            gender = data['gender'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            education = data['education'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            race = data['race'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            age = data['age'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            income = data['income'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "            outputs = self.model(input_ids, attention_mask, gender, education, race, age, income)\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "            if i % 50 == 0:\n",
    "                loss_step = tr_loss / (i+1)\n",
    "                print(f'Training loss per 50 steps: {loss_step}')\n",
    "    \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "        epoch_loss = tr_loss / (i+1)\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'The total loss: {epoch_loss}')\n",
    "\n",
    "    def fit(self, n_epochs, train_loader, dev_loader, output_label, save_model=False):\n",
    "        set_all_seeds(0)\n",
    "        for epoch in range(n_epochs):\n",
    "            self._training_step(epoch, train_loader)\n",
    "\n",
    "            preds = self.evaluate(dataloader=dev_loader, load_model=False)\n",
    "            if self.task == 'empathy':\n",
    "                true = output_label.iloc[:, 0].tolist()\n",
    "            if self.task == 'distress':\n",
    "                true = output_label.iloc[:, 1].tolist()\n",
    "            pearson_r = pearsonr(true, preds)\n",
    "            print(f'Pearson r: {pearson_r}')\n",
    "            \n",
    "            if save_model and (pearson_r > self.best_pearson_r):\n",
    "                self.best_pearson_r = pearson_r   \n",
    "                torch.save(self.model.state_dict(), 'roberta-empathy.pth')\n",
    "                print(\"Saved the model in epoch \" + str(epoch+1))\n",
    "\n",
    "    def evaluate(self, dataloader, load_model=False):\n",
    "        if load_model:\n",
    "            self.model.load_state_dict(torch.load('roberta-empathy.pth'))\n",
    "    \n",
    "        pred = torch.empty((len(dataloader.dataset), 1), device=self.device) # len(self.dev_loader.dataset) --> # of samples\n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            for data in dataloader:\n",
    "                input_ids = data['input_ids'].to(self.device, dtype=torch.long)\n",
    "                attention_mask = data['attention_mask'].to(self.device, dtype=torch.long)\n",
    "                # token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                gender = data['gender'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                education = data['education'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                race = data['race'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                age = data['age'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                income = data['income'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "        \n",
    "                outputs = model(input_ids, attention_mask, gender, education, race, age, income)\n",
    "\n",
    "                batch_size = outputs.shape[0]\n",
    "                pred[idx:idx+batch_size, :] = outputs\n",
    "                idx += batch_size\n",
    "            \n",
    "        return [float(k) for k in pred]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80e65bb5-f377-4664-8720-b18276ee77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "lr = 1e-05\n",
    "checkpoint = 'roberta-base'\n",
    "task = ['empathy']\n",
    "\n",
    "train_file = './data/PREPROCESSED-WS22-WS23-train.tsv'\n",
    "\n",
    "# WASSA 2022\n",
    "dev_file = './data/PREPROCESSED-WS22-dev.tsv'\n",
    "dev_label_file = './data/WASSA22/goldstandard_dev_2022.tsv'\n",
    "test_file = './data/PREPROCESSED-WS22-test.tsv'\n",
    "\n",
    "# WASSA 2023\n",
    "# dev_file = './data/PREPROCESSED-WS23-dev.tsv'\n",
    "# dev_label_file = './data/WASSA23/goldstandard_dev.tsv'\n",
    "# test_file = './data/PREPROCESSED-WS23-test.tsv'\n",
    "\n",
    "demog = ['gender', 'education', 'race', 'age', 'income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc69ec6b-9bda-44f5-90b4-d02754f65d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    task=task,\n",
    "    checkpoint=checkpoint,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    feature_to_tokenise=['essay']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "871e3d4f-df64-42e0-8061-ac862da355de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = data_module.dataloader(file=train_file, send_label=True, shuffle=True)\n",
    "dev_loader = data_module.dataloader(file=dev_file, send_label=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd069104-06d8-40b6-9138-b95bba95be5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaRegressor(checkpoint=checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    task='empathy',\n",
    "    model=model,\n",
    "    lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0d861b3-051d-4d8b-80fa-b8e2c3551b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_label = pd.read_csv(dev_label_file, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4bbeb466-3f4c-4364-bd13-d0ceb0707bbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 50 steps: 10.673906326293945\n",
      "Training loss per 50 steps: 14.912511395473107\n",
      "Training loss per 50 steps: 12.085124150361166\n",
      "Training loss per 50 steps: 9.599270264833969\n",
      "Training loss per 50 steps: 8.153820287528916\n",
      "Training loss per 50 steps: 7.2366881674504375\n",
      "Training loss per 50 steps: 6.677560860136419\n",
      "Epoch: 1\n",
      "The total loss: 6.407750789324442\n",
      "Pearson r: 0.056\n",
      "Training loss per 50 steps: 4.898565292358398\n",
      "Training loss per 50 steps: 3.520465511901706\n",
      "Training loss per 50 steps: 3.550694823560148\n",
      "Training loss per 50 steps: 3.605579699309456\n",
      "Training loss per 50 steps: 3.5889854322915054\n",
      "Training loss per 50 steps: 3.589391334954486\n",
      "Training loss per 50 steps: 3.6055012055211684\n",
      "Epoch: 2\n",
      "The total loss: 3.6328717337413265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 47\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, n_epochs, train_loader, dev_loader, output_label, save_model)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step(epoch, train_loader)\n\u001b[0;32m---> 47\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     true \u001b[38;5;241m=\u001b[39m output_label\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     49\u001b[0m     pearson_r \u001b[38;5;241m=\u001b[39m pearsonr(true, preds)\n",
      "Cell \u001b[0;32mIn[40], line 67\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, dataloader, load_model)\u001b[0m\n\u001b[1;32m     65\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 67\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(n_epochs=8, train_loader=train_loader, dev_loader=dev_loader,\n",
    "            output_label=dev_label, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78d5d6d3-b64d-4ffa-80bb-e20306380e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader = data_module.dataloader(file=test_file, send_label=False, shuffle=False)\n",
    "pred = trainer.evaluate(dataloader=test_loader, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd902148-f104-4813-85bf-e0f52010023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'emp': pred, 'dis': pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80e697ba-1bf9-4787-9202-c15289b714e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emp</th>\n",
       "      <th>dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.743536</td>\n",
       "      <td>4.743536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.375017</td>\n",
       "      <td>5.375017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.558261</td>\n",
       "      <td>5.558261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.441684</td>\n",
       "      <td>4.441684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.513950</td>\n",
       "      <td>4.513950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.794048</td>\n",
       "      <td>5.794048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.106700</td>\n",
       "      <td>5.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.298420</td>\n",
       "      <td>3.298420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4.998303</td>\n",
       "      <td>4.998303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.430133</td>\n",
       "      <td>5.430133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emp       dis\n",
       "0   4.743536  4.743536\n",
       "1   5.375017  5.375017\n",
       "2   5.558261  5.558261\n",
       "3   4.441684  4.441684\n",
       "4   4.513950  4.513950\n",
       "..       ...       ...\n",
       "95  5.794048  5.794048\n",
       "96  5.106700  5.106700\n",
       "97  3.298420  3.298420\n",
       "98  4.998303  4.998303\n",
       "99  5.430133  5.430133\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1eda740e-fd94-4f54-a9be-01774c5abc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('./tmp/predictions_EMP.tsv', sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42400f7c-37fa-419c-b125-dba6168f7ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d8c60-058b-4323-973d-5b208b9fe1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = dev_label.iloc[:,0].tolist()\n",
    "pearsonr(true, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
