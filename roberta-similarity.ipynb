{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94915d97-0f63-4805-b4a3-66e94b3c5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "os.chdir(\"/g/data/jr19/rh2942/text-empathy/\")\n",
    "from evaluation import pearsonr\n",
    "from utils.utils import plot, get_device, set_all_seeds\n",
    "from utils.common import EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf579713-872e-41d6-8a8c-8f14a6e50498",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # due to huggingface warning\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0671f-fa8c-485b-a803-c3ef6139f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.607\n",
    "'''\n",
    "class RobertaRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(RobertaRegressor, self).__init__()\n",
    "        \n",
    "        self.transformer_article = AutoModel.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "        self.transformer_essay = AutoModel.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(768*8+1+5, 512), nn.Tanh(), nn.Dropout(0.2)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.Tanh(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def _get_embeddings(self, output):\n",
    "        layers = [-4, -3, -2, -1] #last four hidden states\n",
    "        states = output.hidden_states\n",
    "        embedding = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "        # print(embedding.shape)\n",
    "        return embedding[:, 0] #return only CLS token's embedding\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_article=None,\n",
    "        attention_mask_article=None,\n",
    "        input_ids_essay=None,\n",
    "        attention_mask_essay=None,\n",
    "        gender=None,\n",
    "        education=None,\n",
    "        race=None,\n",
    "        age=None,\n",
    "        income=None\n",
    "    ):\n",
    "\n",
    "        output_article = self.transformer_article(\n",
    "            input_ids= input_ids_article,\n",
    "            attention_mask=attention_mask_article,\n",
    "        )\n",
    "\n",
    "        output_essay = self.transformer_essay(\n",
    "            input_ids= input_ids_essay,\n",
    "            attention_mask=attention_mask_essay,\n",
    "        )\n",
    "\n",
    "        cosine = nn.functional.cosine_similarity(self._get_embeddings(output_article),\n",
    "                                                self._get_embeddings(output_essay), dim=1).view(-1, 1)\n",
    "\n",
    "        output_article = torch.cat([output_article.hidden_states[i] for i in [-4, -3, -2, -1]], dim=-1) #shape: (batch_size, seq_length, 768*4)\n",
    "        output_essay = torch.cat([output_essay.hidden_states[i] for i in [-4, -3, -2, -1]], dim=-1) #shape: (batch_size, seq_length, 768*4)\n",
    "        output = torch.cat((output_article[:, 0, :], output_essay[:, 0, :], cosine), dim=1)\n",
    "        # output = torch.cat((output_article.last_hidden_state[:, 0, :], output_essay.last_hidden_state[:, 0, :], cosine), dim=1)\n",
    "        output = torch.cat([output, gender, education, race, age, income], 1)\n",
    "\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6985953e-7fae-4c20-bebe-de6a345c50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(RobertaRegressor, self).__init__()\n",
    "        \n",
    "        self.transformer_article = AutoModel.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "        self.transformer_essay = AutoModelForSequenceClassification.from_pretrained(checkpoint, output_hidden_states=True, num_labels=768)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(768, 512), nn.Tanh(), nn.Dropout(0.2)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512+5+1, 256), nn.Tanh(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def _get_embeddings(self, output):\n",
    "        layers = [-4, -3, -2, -1] #last four hidden states\n",
    "        states = output.hidden_states\n",
    "        embedding = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "        # print(embedding.shape)\n",
    "        return embedding[:, 0] #return only CLS token's embedding\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_article=None,\n",
    "        attention_mask_article=None,\n",
    "        input_ids_essay=None,\n",
    "        attention_mask_essay=None,\n",
    "        gender=None,\n",
    "        education=None,\n",
    "        race=None,\n",
    "        age=None,\n",
    "        income=None\n",
    "    ):\n",
    "\n",
    "        output_article = self.transformer_article(\n",
    "            input_ids= input_ids_article,\n",
    "            attention_mask=attention_mask_article,\n",
    "        )\n",
    "\n",
    "        output_essay = self.transformer_essay(\n",
    "            input_ids= input_ids_essay,\n",
    "            attention_mask=attention_mask_essay,\n",
    "        )\n",
    "\n",
    "        cosine = nn.functional.cosine_similarity(self._get_embeddings(output_article),\n",
    "                                                self._get_embeddings(output_essay), dim=1).view(-1, 1)\n",
    "\n",
    "        # output_article = torch.cat([output_article.hidden_states[i] for i in [-4, -3, -2, -1]], dim=-1) #shape: (batch_size, seq_length, 768*4)\n",
    "        # output_essay = torch.cat([output_essay.hidden_states[i] for i in [-4, -3, -2, -1]], dim=-1) #shape: (batch_size, seq_length, 768*4)\n",
    "        # output = torch.cat((output_article[:, 0, :], output_essay[:, 0, :], cosine), dim=1)\n",
    "        # output = torch.cat((output_article.pooler_output, output_essay.pooler_output, cosine), dim=1)\n",
    "\n",
    "        # output = self.fc1(torch.cat((output_essay.logits, cosine), 1))\n",
    "        output = self.fc1(output_essay.logits)\n",
    "        output = torch.cat([output, gender, education, race, age, income, cosine], 1)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee314462-a714-4cea-80bb-82097bb625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, task, checkpoint, batch_size, feature_to_tokenise, seed):\n",
    "\n",
    "        self.task = task\n",
    "        self.checkpoint = checkpoint\n",
    "        self.batch_size = batch_size\n",
    "        self.tokeniser = AutoTokenizer.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokeniser)\n",
    "        self.feature_to_tokenise = feature_to_tokenise # to tokenise function\n",
    "        self.seed = seed\n",
    "\n",
    "        assert len(self.task) == 1, 'task must be a list with one element'\n",
    "    \n",
    "    def _process_raw(self, path, send_label):\n",
    "        data = pd.read_csv(path, sep='\\t')\n",
    "    \n",
    "        if send_label:\n",
    "            text = data[self.feature_to_tokenise + self.task]\n",
    "        else:\n",
    "            text = data[self.feature_to_tokenise]\n",
    "\n",
    "        demog = ['gender', 'education', 'race', 'age', 'income']\n",
    "        data_demog = data[demog]\n",
    "        scaler = MinMaxScaler()\n",
    "        data_demog = pd.DataFrame(\n",
    "            scaler.fit_transform(data_demog),\n",
    "            columns=demog\n",
    "        )\n",
    "        data = pd.concat([text, data_demog], axis=1) \n",
    "        return data\n",
    "\n",
    "    def _tokeniser_fn(self, sentence):\n",
    "        if len(self.feature_to_tokenise) == 1: # only one feature\n",
    "            return self.tokeniser(sentence[self.feature_to_tokenise[0]], truncation=True)\n",
    "        # otherwise tokenise a pair of sentence\n",
    "        return self.tokeniser(sentence[self.feature_to_tokenise[0]], sentence[self.feature_to_tokenise[1]], truncation=True)\n",
    "\n",
    "    def _process_input(self, file, send_label):\n",
    "        data = self._process_raw(path=file, send_label=send_label)\n",
    "        data = Dataset.from_pandas(data, preserve_index=False) # convert to huggingface dataset\n",
    "        data = data.map(self._tokeniser_fn, batched=True, remove_columns=self.feature_to_tokenise) # tokenise\n",
    "        data = data.with_format('torch')\n",
    "        return data\n",
    "\n",
    "    # taken from https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    def _seed_worker(self, worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)     \n",
    "\n",
    "    def dataloader(self, file, send_label, shuffle):\n",
    "        data = self._process_input(file=file, send_label=send_label)\n",
    "\n",
    "        # making sure the shuffling is reproducible\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.seed)\n",
    "        \n",
    "        return DataLoader(\n",
    "            data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=12,\n",
    "            worker_init_fn=self._seed_worker,\n",
    "            generator=g\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095ee97b-ccaf-410a-bd26-5067564e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, task, model, lr, n_epochs, train_loader_article, train_loader_essay,\n",
    "                 dev_loader_article, dev_loader_essay, dev_label_file, device_id=0):\n",
    "        self.device = get_device(device_id)\n",
    "        self.task = task\n",
    "        self.model = model.to(self.device)\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.train_loader_article = train_loader_article\n",
    "        self.train_loader_essay = train_loader_essay\n",
    "        self.dev_loader_article = dev_loader_article\n",
    "        self.dev_loader_essay = dev_loader_essay\n",
    "        self.dev_label_file = dev_label_file\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimiser = torch.optim.AdamW(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-06,\n",
    "            weight_decay=0.1\n",
    "        )\n",
    "\n",
    "        n_training_step = self.n_epochs*len(self.train_loader_article)\n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=self.optimiser,\n",
    "            num_warmup_steps=0.06*n_training_step,\n",
    "            num_training_steps=n_training_step\n",
    "        )\n",
    "        \n",
    "        self.best_pearson_r = -1.0 # initiliasation\n",
    "        self.early_stopper = EarlyStopper(patience=3, min_delta=0.01)\n",
    "        \n",
    "        assert len(self.task) == 1, 'task must be a list with one element'\n",
    "\n",
    "    def _training_step(self):\n",
    "        tr_loss = 0.0\n",
    "        self.model.train()\n",
    "    \n",
    "        for (data_article, data_essay) in zip(self.train_loader_article, self.train_loader_essay):\n",
    "            input_ids_article = data_article['input_ids'].to(self.device, dtype=torch.long)\n",
    "            attention_mask_article = data_article['attention_mask'].to(self.device, dtype=torch.long)\n",
    "            \n",
    "            input_ids_essay = data_essay['input_ids'].to(self.device, dtype=torch.long)\n",
    "            attention_mask_essay = data_essay['attention_mask'].to(self.device, dtype=torch.long)\n",
    "            \n",
    "            assert (data_article[self.task[0]].detach().numpy() == data_essay[self.task[0]].detach().numpy()).all(), 'Ground truth is different between encoders'\n",
    "            targets = data_essay[self.task[0]].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            \n",
    "            gender = data_essay['gender'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            education = data_essay['education'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            race = data_essay['race'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            age = data_essay['age'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "            income = data_essay['income'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "            outputs = self.model(\n",
    "                input_ids_article=input_ids_article,\n",
    "                attention_mask_article=attention_mask_article,\n",
    "                input_ids_essay=input_ids_essay,\n",
    "                attention_mask_essay=attention_mask_essay,\n",
    "                gender=gender,\n",
    "                education=education,\n",
    "                race=race,\n",
    "                age=age,\n",
    "                income=income\n",
    "            )\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "            self.optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "        epoch_loss = tr_loss / len(train_loader_essay)\n",
    "        print(f'Train loss: {epoch_loss}')\n",
    "\n",
    "    def fit(self, save_model=False, gpt_anno=True):\n",
    "        '''\n",
    "        gpt_anno: the dev label is differently placed as compared no normal datasets\n",
    "        '''\n",
    "        if gpt_anno:\n",
    "            dev_label = pd.read_csv(self.dev_label_file, sep='\\t')\n",
    "            true = dev_label.loc[:, 'empathy'].tolist()\n",
    "        else:\n",
    "            dev_label = pd.read_csv(self.dev_label_file, sep='\\t', header=None)\n",
    "            true = dev_label.iloc[:, 0].tolist()\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            print(f'Epoch: {epoch+1}')\n",
    "            self._training_step()\n",
    "\n",
    "            preds = self.evaluate(dataloader_article=self.dev_loader_article,\n",
    "                                  dataloader_essay=self.dev_loader_essay,\n",
    "                                  load_model=False)\n",
    "            \n",
    "            pearson_r = pearsonr(true, preds)\n",
    "            print(f'Pearson r: {pearson_r}')\n",
    "            \n",
    "            val_loss = self.loss_fn(torch.tensor(preds), torch.tensor(true))\n",
    "            print('Validation loss:', val_loss.item())\n",
    "            \n",
    "            if self.early_stopper.early_stop(val_loss):\n",
    "                break\n",
    "\n",
    "            if (pearson_r > self.best_pearson_r):\n",
    "                self.best_pearson_r = pearson_r            \n",
    "                if save_model:\n",
    "                    torch.save(self.model.state_dict(), 'roberta-baseline.pth')\n",
    "                    print(\"Saved the model in epoch \" + str(epoch+1))\n",
    "            \n",
    "            print(f'Best dev set Pearson r: {self.best_pearson_r}\\n')\n",
    "\n",
    "    def evaluate(self, dataloader_article, dataloader_essay, load_model=False):\n",
    "        if load_model:\n",
    "            self.model.load_state_dict(torch.load('roberta-baseline.pth'))\n",
    "    \n",
    "        pred = torch.empty((len(dataloader_essay.dataset), 1), device=self.device) # len(self.dev_loader.dataset) --> # of samples\n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            for (data_article, data_essay) in zip(dataloader_article, dataloader_essay):\n",
    "                input_ids_article = data_article['input_ids'].to(self.device, dtype=torch.long)\n",
    "                attention_mask_article = data_article['attention_mask'].to(self.device, dtype=torch.long)\n",
    "                \n",
    "                input_ids_essay = data_essay['input_ids'].to(self.device, dtype=torch.long)\n",
    "                attention_mask_essay = data_essay['attention_mask'].to(self.device, dtype=torch.long)\n",
    "                \n",
    "                assert (data_article['gender'].detach().numpy() == data_essay['gender'].detach().numpy()).all(), 'Sample value is different between encoders'\n",
    "                \n",
    "                gender = data_essay['gender'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                education = data_essay['education'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                race = data_essay['race'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                age = data_essay['age'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "                income = data_essay['income'].to(self.device, dtype=torch.float).view(-1, 1)\n",
    "        \n",
    "                outputs = self.model(\n",
    "                    input_ids_article=input_ids_article,\n",
    "                    attention_mask_article=attention_mask_article,\n",
    "                    input_ids_essay=input_ids_essay,\n",
    "                    attention_mask_essay=attention_mask_essay,\n",
    "                    gender=gender,\n",
    "                    education=education,\n",
    "                    race=race,\n",
    "                    age=age,\n",
    "                    income=income\n",
    "                )\n",
    "                \n",
    "                batch_size = outputs.shape[0]\n",
    "                pred[idx:idx+batch_size, :] = outputs\n",
    "                idx += batch_size\n",
    "            \n",
    "        return [float(k) for k in pred]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e65bb5-f377-4664-8720-b18276ee77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'roberta-base'\n",
    "task = ['empathy']\n",
    "feature_to_tokenise=['demographic_essay', 'article']\n",
    "# feature_to_tokenise=['demographic', 'essay']\n",
    "# feature_to_tokenise=['demographic_essay']\n",
    "seed = 0\n",
    "\n",
    "################# COMBINED TRAIN FILE ##############\n",
    "train_file = './data/WS22-WS23-train-gpt.tsv'\n",
    "# train_file = './data/essay-train-ws22-ws23.tsv'\n",
    "# train_file = './data/PREPROCESSED-WS22-WS23-train.tsv'\n",
    "# train_file = './data/COMBINED-PREPROCESSED-PARAPHRASED-WS22-WS23-train.tsv'\n",
    "\n",
    "################# WASSA 2022 ####################\n",
    "# dev_file = './data/PREPROCESSED-WS22-dev.tsv'\n",
    "# dev_label_file = './data/WASSA22/goldstandard_dev_2022.tsv'\n",
    "# test_file = './data/PREPROCESSED-WS22-test.tsv'\n",
    "\n",
    "##### GPT annotation\n",
    "# train_file = './data/WS22-train-gpt.tsv'\n",
    "# dev_file = './data/WS22-dev-gpt.tsv'\n",
    "# dev_label_file = './data/WS22-dev-gpt.tsv'\n",
    "\n",
    "\n",
    "################# WASSA 2023 ####################\n",
    "# dev_file = './data/PREPROCESSED-WS23-dev.tsv'\n",
    "# dev_label_file = './data/WASSA23/goldstandard_dev.tsv'\n",
    "test_file = './data/PREPROCESSED-WS23-test.tsv'\n",
    "\n",
    "##### GPT annotation\n",
    "# train_file = './data/WS23-train-gpt.tsv'\n",
    "dev_file = './data/WS23-dev-gpt.tsv'\n",
    "dev_label_file = './data/WS23-dev-gpt.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc37325e-06a3-4967-a1e4-07c638ffa781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jr19/rh2942/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jr19/rh2942/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jr19/rh2942/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jr19/rh2942/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jr19/rh2942/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 14.017223392833364\n",
      "Pearson r: 0.058\n",
      "Validation loss: 2.689617156982422\n",
      "Saved the model in epoch 1\n",
      "Best dev set Pearson r: 0.058\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 3.377618338844993\n",
      "Pearson r: 0.529\n",
      "Validation loss: 1.8220162391662598\n",
      "Saved the model in epoch 2\n",
      "Best dev set Pearson r: 0.529\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 2.092752405730161\n",
      "Pearson r: 0.64\n",
      "Validation loss: 1.2241322994232178\n",
      "Saved the model in epoch 3\n",
      "Best dev set Pearson r: 0.64\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 1.4857874155044555\n",
      "Pearson r: 0.701\n",
      "Validation loss: 1.1257109642028809\n",
      "Saved the model in epoch 4\n",
      "Best dev set Pearson r: 0.701\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 1.2184921373020519\n",
      "Pearson r: 0.708\n",
      "Validation loss: 1.0383673906326294\n",
      "Saved the model in epoch 5\n",
      "Best dev set Pearson r: 0.708\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 0.9412215841538978\n",
      "Pearson r: 0.73\n",
      "Validation loss: 0.9889430403709412\n",
      "Saved the model in epoch 6\n",
      "Best dev set Pearson r: 0.73\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 0.7425705148415132\n",
      "Pearson r: 0.714\n",
      "Validation loss: 1.0883115530014038\n",
      "Best dev set Pearson r: 0.73\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 0.6066604309461333\n",
      "Pearson r: 0.72\n",
      "Validation loss: 1.1060316562652588\n",
      "Best dev set Pearson r: 0.73\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 0.5273135960553632\n",
      "Pearson r: 0.726\n",
      "Validation loss: 1.071838140487671\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds(seed)\n",
    "\n",
    "data_module_article = DataModule(\n",
    "    task=task,\n",
    "    checkpoint=checkpoint,\n",
    "    batch_size=16,\n",
    "    feature_to_tokenise=[feature_to_tokenise[1]],\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "data_module_essay = DataModule(\n",
    "    task=task,\n",
    "    checkpoint=checkpoint,\n",
    "    batch_size=16,\n",
    "    feature_to_tokenise=[feature_to_tokenise[0]],\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "train_loader_article = data_module_article.dataloader(file=train_file, send_label=True, shuffle=True)\n",
    "train_loader_essay = data_module_essay.dataloader(file=train_file, send_label=True, shuffle=True)\n",
    "dev_loader_article = data_module_article.dataloader(file=dev_file, send_label=False, shuffle=False)\n",
    "dev_loader_essay = data_module_essay.dataloader(file=dev_file, send_label=False, shuffle=False)\n",
    "\n",
    "model = RobertaRegressor(checkpoint=checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    lr=1e-5,\n",
    "    n_epochs=10,\n",
    "    train_loader_article=train_loader_article,\n",
    "    train_loader_essay=train_loader_essay,\n",
    "    dev_loader_article=dev_loader_article,\n",
    "    dev_loader_essay=dev_loader_essay,\n",
    "    dev_label_file=dev_label_file,\n",
    "    device_id=0\n",
    ")\n",
    "\n",
    "trainer.fit(save_model=True, gpt_anno=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15c9c9-d937-4f8a-825f-7ef384dad0f3",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78d5d6d3-b64d-4ffa-80bb-e20306380e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jr19/rh2942/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jr19/rh2942/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emp</th>\n",
       "      <th>dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.631469</td>\n",
       "      <td>3.631469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.198637</td>\n",
       "      <td>6.198637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.589288</td>\n",
       "      <td>4.589288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.492352</td>\n",
       "      <td>2.492352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.224233</td>\n",
       "      <td>6.224233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6.429137</td>\n",
       "      <td>6.429137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.013251</td>\n",
       "      <td>5.013251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5.388442</td>\n",
       "      <td>5.388442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6.416263</td>\n",
       "      <td>6.416263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.225664</td>\n",
       "      <td>5.225664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emp       dis\n",
       "0   3.631469  3.631469\n",
       "1   6.198637  6.198637\n",
       "2   4.589288  4.589288\n",
       "3   2.492352  2.492352\n",
       "4   6.224233  6.224233\n",
       "..       ...       ...\n",
       "95  6.429137  6.429137\n",
       "96  5.013251  5.013251\n",
       "97  5.388442  5.388442\n",
       "98  6.416263  6.416263\n",
       "99  5.225664  5.225664\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader_article = data_module_article.dataloader(file=test_file, send_label=False, shuffle=False)\n",
    "test_loader_essay = data_module_essay.dataloader(file=test_file, send_label=False, shuffle=False)\n",
    "pred = trainer.evaluate(dataloader_article=test_loader_article,\n",
    "                        dataloader_essay=test_loader_essay, load_model=True)\n",
    "pred_df = pd.DataFrame({'emp': pred, 'dis': pred}) # we're not predicting distress, just aligning with submission system\n",
    "pred_df.to_csv('./tmp/predictions_EMP.tsv', sep='\\t', index=None, header=None)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7e7b0-560e-4f33-8e94-b74904125d7d",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ccd8e2-f398-46e3-8c29-b1a6c858c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super(RobertaRegressor, self).__init__()\n",
    "        \n",
    "        self.transformer_article = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=768)\n",
    "        self.transformer_essay = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=768)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(768+768+1, 512), nn.Tanh(), nn.Dropout(0.2)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512+5, 256), nn.Tanh(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_article=None,\n",
    "        attention_mask_article=None,\n",
    "        input_ids_essay=None,\n",
    "        attention_mask_essay=None,\n",
    "        gender=None,\n",
    "        education=None,\n",
    "        race=None,\n",
    "        age=None,\n",
    "        income=None\n",
    "    ):\n",
    "\n",
    "        output_article = self.transformer_article(\n",
    "            input_ids= input_ids_article,\n",
    "            attention_mask=attention_mask_article,\n",
    "        )\n",
    "\n",
    "        output_essay = self.transformer_essay(\n",
    "            input_ids= input_ids_essay,\n",
    "            attention_mask=attention_mask_essay,\n",
    "        )\n",
    "\n",
    "        cosine = nn.functional.cosine_similarity(output_article.logits,\n",
    "                                                output_essay.logits, dim=1).view(-1, 1)\n",
    "\n",
    "        output = torch.cat((output_article.logits, output_essay.logits, cosine), dim=1)\n",
    "\n",
    "        output = self.fc1(output)\n",
    "        output = torch.cat([output, gender, education, race, age, income], 1)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
