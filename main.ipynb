{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d78693-aff6-4484-9689-b92b91844b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_scheduler\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd96ca12-af79-4a12-8f47-218e424f8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"./text-empathy/\")\n",
    "from evaluation import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561a3d26-7b1e-4458-8dd8-177ee1c49187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, y2=None, xlabel=None, ylabel=None, legend=[], save=False, filename=None):\n",
    "    \"\"\"Plot data points\"\"\"\n",
    "    plt.style.use(['science'])\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    \n",
    "    ax.plot(x,y)\n",
    "    if y2 is not None:\n",
    "        ax.plot(x, y2)\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(legend)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(fname=filename+'.pdf', format = 'pdf', bbox_inches='tight')\n",
    "        print(f\"Saved as {filename}.pdf\")\n",
    "        \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "623f0a4d-9e54-42e5-bfe3-4c80f728a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_1 = 'empathy'\n",
    "task_2 = 'distress'\n",
    "\n",
    "checkpoint = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "train_file = \"./data/PREPROCESSED-essay-train.csv\"\n",
    "test_file = \"./data/PREPROCESSED-essay-dev.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf20e3-73ad-4f90-a66f-17fc8aff718d",
   "metadata": {},
   "source": [
    "# Multi-task learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c10456bb-383e-4016-8335-82603b3f3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = 'demographic_essay'\n",
    "feature_2 = 'article'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38e29437-8b87-46af-b698-ae5d431c267e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, task_1, task_2, feature_to_tokenise, train, batch_size):\n",
    "\n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_to_tokenise], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[[feature_to_tokenise, task_1, task_2]]\n",
    "    else:\n",
    "        chosen_data = input_data[[feature_to_tokenise]]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = [feature_to_tokenise])\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task_1, \"labels_1\") # more meaningful\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task_2, \"labels_2\")\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "           \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=train, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "256b6524-9ef8-440d-9a32-8f8803166c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(BiEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(1536, 768) #768+768 = 1536 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        \n",
    "        self.fc4_1 = nn.Linear(256, 1) # regression problem\n",
    "        self.fc4_2 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_f1=None,\n",
    "        attention_mask_f1=None,\n",
    "        input_ids_f2=None,\n",
    "        attention_mask_f2=None,\n",
    "        labels_1=None,\n",
    "        labels_2=None\n",
    "    ):\n",
    "        outputs_f1 = self.transformer(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls), dim=1) # shape: (batch_size, 1536)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        \n",
    "        logits_1 = self.fc4_1(X)\n",
    "        logits_2 = self.fc4_2(X)\n",
    "        \n",
    "        loss_1 = None\n",
    "        if labels_1 is not None:\n",
    "            loss_1 = F.mse_loss(logits_1.view(-1), labels_1.view(-1))\n",
    "\n",
    "        loss_2 = None\n",
    "        if labels_2 is not None:\n",
    "            loss_2 = F.mse_loss(logits_2.view(-1), labels_2.view(-1))\n",
    "        \n",
    "        return (\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_1,\n",
    "                logits=logits_1\n",
    "            ),\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_2,\n",
    "                logits=logits_2\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e1fb28b-edc0-4a29-a3a0-138386504c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = BiEncoderTransformer()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader_f1 = load_tokenised_data(\n",
    "        filename=train_file, task_1=task_1, task_2=task_2, feature_to_tokenise=feature_1, train=True, batch_size=batch_size\n",
    "    )\n",
    "    trainloader_f2 = load_tokenised_data(\n",
    "        filename=train_file, task_1=task_1, task_2=task_2, feature_to_tokenise=feature_2, train=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # evaluation data loader in train mode to access labels\n",
    "    testloader_f1 = load_tokenised_data(\n",
    "        filename=test_file, task_1=task_1, task_2=task_2, feature_to_tokenise=feature_1, train=True, batch_size=batch_size\n",
    "    )\n",
    "    testloader_f2 = load_tokenised_data(\n",
    "        filename=test_file, task_1=task_1, task_2=task_2, feature_to_tokenise=feature_2, train=True, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader_f1)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    wgt_1=1\n",
    "    wgt_2=1\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2) in zip(trainloader_f1, trainloader_f2):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels_1': batch_f1['labels_1'].to(device), #batch_f2 labels should be the same\n",
    "                'labels_2': batch_f1['labels_2'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            (outputs_1, outputs_2) = model(**batch)\n",
    "            loss_1 = outputs_1.loss\n",
    "            loss_2 = outputs_2.loss\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader_f1)) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2) in zip(testloader_f1, testloader_f2):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels_1': batch_f1['labels_1'].to(device), #batch_f2 labels should be the same\n",
    "                'labels_2': batch_f1['labels_2'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                (outputs_1, outputs_2) = model(**batch)\n",
    "\n",
    "            batch_pred_1 = [item for sublist in outputs_1.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred_1)\n",
    "\n",
    "            y_true.extend((batch_f1['labels_1'].tolist())) #batch_f2 labels should be the same\n",
    "\n",
    "            loss_1 = outputs_1.loss.item()\n",
    "            loss_2 = outputs_2.loss.item()\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "\n",
    "            total_loss += loss\n",
    "        \n",
    "        val_loss.append(total_loss / len(testloader_f1))\n",
    "        \n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf4f8a06-776f-4c9e-b1e5-d3bb682cd14d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.068\n",
      "pearson_r: 0.244\n",
      "pearson_r: 0.466\n",
      "pearson_r: 0.667\n",
      "pearson_r: 0.662\n",
      "pearson_r: 0.688\n",
      "pearson_r: 0.69\n",
      "pearson_r: 0.695\n",
      "pearson_r: 0.715\n",
      "pearson_r: 0.722\n",
      "pearson_r: 0.733\n",
      "pearson_r: 0.737\n",
      "pearson_r: 0.742\n",
      "pearson_r: 0.738\n",
      "pearson_r: 0.741\n",
      "pearson_r: 0.737\n",
      "pearson_r: 0.738\n",
      "pearson_r: 0.737\n",
      "pearson_r: 0.737\n",
      "pearson_r: 0.737\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test(lr=1e-5, batch_size=8, n_epochs=20, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba39ac-c3c1-47be-98ae-c2752c8ff953",
   "metadata": {},
   "source": [
    "# Multi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61c1dbc1-f0d0-4b4d-aeb1-fc3f483d670e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, task, feature_to_tokenise, train, batch_size):\n",
    "\n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_to_tokenise], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[[feature_to_tokenise, task]]\n",
    "    else:\n",
    "        chosen_data = input_data[[feature_to_tokenise]]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = [feature_to_tokenise])\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task, \"labels\") # as huggingface requires\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "           \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=train, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb1835-6b60-4fe0-90da-ad8f2d7b1dc4",
   "metadata": {},
   "source": [
    "## Tri-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e5bcde6-6710-4da2-9370-472eb77b1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = 'demographic'\n",
    "feature_2 = 'essay'\n",
    "feature_3 = 'article'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f17afdd1-eb47-49f4-add8-a9da34160099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(TriEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer_f1 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.transformer_f2 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.transformer_f3 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(2304, 768) #768+768+768 = 2304 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        self.fc4 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer_f1.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.transformer_f2.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.transformer_f3.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer_f1.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                for layer in self.transformer_f2.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                for layer in self.transformer_f3.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids_f1=None, attention_mask_f1=None, input_ids_f2=None, attention_mask_f2=None, input_ids_f3=None, attention_mask_f3=None, labels=None):\n",
    "        outputs_f1 = self.transformer_f1(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer_f2(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        outputs_f3 = self.transformer_f3(\n",
    "            input_ids = input_ids_f3,\n",
    "            attention_mask = attention_mask_f3,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "        outputs_f3_last_state = outputs_f3[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "        outputs_f3_cls = outputs_f3_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls, outputs_f3_cls), dim=1) # shape: (batch_size, 768*3)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        logits = self.fc4(X)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            # hidden_states=outputs.hidden_states,\n",
    "            # attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7ad12cdb-7d16-44c3-8ce2-601d4a34431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(task=task_1, lr=1e-5, batch_size=8, n_epochs=3, seed=1, test=False):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = TriEncoderTransformer()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader_f1 = load_tokenised_data(\n",
    "        filename=train_file, task=task, feature_to_tokenise=feature_1, train=True, batch_size=batch_size\n",
    "    )\n",
    "    trainloader_f2 = load_tokenised_data(\n",
    "        filename=train_file, task=task, feature_to_tokenise=feature_2, train=True, batch_size=batch_size\n",
    "    )\n",
    "    trainloader_f3 = load_tokenised_data(\n",
    "        filename=train_file, task=task, feature_to_tokenise=feature_3, train=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    if not test:\n",
    "        # evaluation data loader in train mode to access labels\n",
    "        testloader_f1 = load_tokenised_data(\n",
    "            filename=test_file, task=task, feature_to_tokenise=feature_1, train=True, batch_size=batch_size\n",
    "        )\n",
    "        testloader_f2 = load_tokenised_data(\n",
    "            filename=test_file, task=task, feature_to_tokenise=feature_2, train=True, batch_size=batch_size\n",
    "        )\n",
    "        testloader_f3 = load_tokenised_data(\n",
    "            filename=test_file, task=task, feature_to_tokenise=feature_3, train=True, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    if test:\n",
    "        testloader_f1 = load_tokenised_data(\n",
    "            filename=test_file, task=task, feature_to_tokenise=feature_1, train=False, batch_size=batch_size\n",
    "        )\n",
    "        testloader_f2 = load_tokenised_data(\n",
    "            filename=test_file, task=task, feature_to_tokenise=feature_2, train=False, batch_size=batch_size\n",
    "        )\n",
    "        testloader_f3 = load_tokenised_data(\n",
    "            filename=test_file, task=task, feature_to_tokenise=feature_3, train=False, batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader_f1)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2, batch_f3) in zip(trainloader_f1, trainloader_f2, trainloader_f3):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "                'attention_mask_f3': batch_f3['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader_f1)) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2, batch_f3) in zip(testloader_f1, testloader_f2, testloader_f3):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "                'attention_mask_f3': batch_f3['attention_mask'].to(device)\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            if not test:\n",
    "                y_true.extend((batch_f1['labels'].tolist())) #batch_f2 labels should be the same\n",
    "                total_loss += outputs.loss.item()\n",
    "        \n",
    "        if not test:\n",
    "            val_loss.append(total_loss / len(testloader_f1))\n",
    "            print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "        \n",
    "    #save the last epoch's prediction \n",
    "    if test:\n",
    "        y_pred_df = pd.DataFrame({task: y_pred})\n",
    "        filename = \"./tmp/predictions_\" + task + \".tsv\"\n",
    "        y_pred_df.to_csv(filename, sep='\\t', header=False, index=False)\n",
    "    \n",
    "    if not test:\n",
    "        # train-test finished\n",
    "        plot(\n",
    "            x=list(range(1, n_epochs+1)),\n",
    "            y=train_loss,\n",
    "            y2=val_loss,\n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss',\n",
    "            legend=['Training loss', 'Validation loss'],\n",
    "            save=False,\n",
    "            filename=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8278e599-f5b2-4c39-8edf-e4536e357472",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./data/PREPROCESSED-essay-train-dev.csv\"\n",
    "test_file = \"./data/PREPROCESSED-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48d974d2-60c3-4c7c-8310-3da78bb59de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test(task=task_1, lr=1e-5, batch_size=8, n_epochs=10, seed=1, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b48677bc-a33d-48c4-a40b-ad987e0bc08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test(task=task_2, lr=1e-5, batch_size=8, n_epochs=10, seed=1, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "92039d69-21a7-43ab-bd6d-b377bdccaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1 = pd.read_csv(\"./tmp/predictions_\" + task_1 + \".tsv\", sep='\\t', header=None)\n",
    "predictions_2 = pd.read_csv(\"./tmp/predictions_\" + task_2 + \".tsv\", sep='\\t', header=None)\n",
    "predictions = pd.concat([predictions_1, predictions_2], axis=1)\n",
    "predictions.to_csv(\"./tmp/predictions_EMP.tsv\", sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d5e6b-55ed-42f1-aee7-c14625b0202e",
   "metadata": {},
   "source": [
    "## Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1d6a54f-2f10-40cc-9099-119fe64c6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = 'demographic_essay'\n",
    "feature_2 = 'article'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2affe2e-f389-4d0c-996a-782b410da1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(BiEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer_f1 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.transformer_f2 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(1536, 768) #768+768 = 1536 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        self.fc4 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer_f1.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.transformer_f2.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer_f1.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                for layer in self.transformer_f2.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids_f1=None, attention_mask_f1=None, input_ids_f2=None, attention_mask_f2=None, labels=None):\n",
    "        outputs_f1 = self.transformer_f1(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer_f1(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls), dim=1) # shape: (batch_size, 1536)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        logits = self.fc4(X)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            # hidden_states=outputs.hidden_states,\n",
    "            # attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d85476bf-7454-4f48-b8dc-665a60ece595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(task=task_1, lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = BiEncoderTransformer()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader_f1 = load_tokenised_data(\n",
    "        filename=train_file, task=task, feature_to_tokenise=feature_1, train=True, batch_size=batch_size\n",
    "    )\n",
    "    trainloader_f2 = load_tokenised_data(\n",
    "        filename=train_file, task=task, feature_to_tokenise=feature_2, train=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # evaluation data loader in train mode to access labels\n",
    "    testloader_f1 = load_tokenised_data(\n",
    "        filename=test_file, task=task, feature_to_tokenise=feature_1, train=True, batch_size=batch_size\n",
    "    )\n",
    "    testloader_f2 = load_tokenised_data(\n",
    "        filename=test_file, task=task, feature_to_tokenise=feature_2, train=True, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader_f1)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2) in zip(trainloader_f1, trainloader_f2):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader_f1)) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2) in zip(testloader_f1, testloader_f2):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            y_true.extend((batch_f1['labels'].tolist())) #batch_f2 labels should be the same\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss.append(total_loss / len(testloader_f1))\n",
    "        \n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f0dba19-dc03-42a2-8243-b3e90c89b1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.033\n",
      "pearson_r: 0.557\n",
      "pearson_r: 0.498\n",
      "pearson_r: 0.599\n",
      "pearson_r: 0.668\n",
      "pearson_r: 0.669\n",
      "pearson_r: 0.69\n",
      "pearson_r: 0.709\n",
      "pearson_r: 0.714\n",
      "pearson_r: 0.734\n",
      "pearson_r: 0.727\n",
      "pearson_r: 0.744\n",
      "pearson_r: 0.735\n",
      "pearson_r: 0.732\n",
      "pearson_r: 0.737\n",
      "pearson_r: 0.742\n",
      "pearson_r: 0.743\n",
      "pearson_r: 0.738\n",
      "pearson_r: 0.739\n",
      "pearson_r: 0.737\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test(task=task_1, lr=1e-5, batch_size=8, n_epochs=20, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa907ad5-bebc-4870-9887-66d3cb1efdf4",
   "metadata": {},
   "source": [
    "# Combined two sequences by SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534441d0-6378-479d-b82f-af1f3abe1316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "#padding=\"longest\" can be deferred to do dynamic padding\n",
    "def tokenise(sentence):\n",
    "    return tokeniser(sentence[feature_1], sentence[feature_2], truncation=True, max_length=512)\n",
    "\n",
    "def load_tokenised_data(filename, task, tokenise_fn, train):\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[[feature_1, feature_2, task]]\n",
    "    else:\n",
    "        chosen_data = input_data[[feature_1, feature_2]]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = [feature_1, feature_2])\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task, \"labels\") # as huggingface requires\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "\n",
    "    return tokenised_hugging_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53f604b-0e21-4091-822c-24165e9fc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(CustomRegressor, self).__init__()\n",
    "\n",
    "        self.transformer_last_hidden_size = 768\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "                num_labels=self.transformer_last_hidden_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(self.transformer_last_hidden_size, 256)\n",
    "        # self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.deberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.deberta.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        # print(outputs.keys())\n",
    "        # will return ['logits', 'hidden_states', 'attentions']\n",
    "\n",
    "        x = self.dropout(outputs.logits) # output.logits aka pooled output from [CLS] token beacuse of AutoModelForSequenceClassification\n",
    "        # x = F.tanh(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "                \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1d482d-a1b9-450a-83fa-586a7eaeec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(task=task, lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = CustomRegressor()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainset = load_tokenised_data(\n",
    "        filename=train_file, task=task, tokenise_fn=tokenise, train=True\n",
    "    )           \n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # evaluation data loader in train mode to access labels\n",
    "    testset = load_tokenised_data(\n",
    "        filename=test_file, task=task, tokenise_fn=tokenise, train=True\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    training_steps = n_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss.append(total_loss / len(trainloader)) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch in testloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "            y_true.extend((batch['labels'].tolist()))\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss.append(total_loss / len(testloader))\n",
    "\n",
    "        # print(y_pred)\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc3c150-e51b-4501-9d47-b05057cf2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n",
      "CustomRegressor(\n",
      "  (transformer): DebertaV2ForSequenceClassification(\n",
      "    (deberta): DebertaV2Model(\n",
      "      (embeddings): DebertaV2Embeddings(\n",
      "        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "      (encoder): DebertaV2Encoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x DebertaV2Layer(\n",
      "            (attention): DebertaV2Attention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): DebertaV2SelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): DebertaV2Intermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DebertaV2Output(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (rel_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (pooler): ContextPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.261\n",
      "pearson_r: 0.563\n",
      "pearson_r: 0.641\n",
      "pearson_r: 0.729\n",
      "pearson_r: 0.725\n",
      "pearson_r: 0.763\n",
      "pearson_r: 0.774\n",
      "pearson_r: 0.777\n",
      "pearson_r: 0.773\n",
      "pearson_r: 0.772\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test(task=task, lr=1e-5, batch_size=8, n_epochs=10, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa154a-d5b7-46c4-a85b-933f2cc38ded",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1be1e38-d79d-4f37-bc00-6bf3ce2983c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(train_file, header=0, index_col=0)\n",
    "pearsonr(data['empathy'], data['distress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d410c466-ce0b-45e5-83f2-7745af9e109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.594"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(test_file, header=0, index_col=0)\n",
    "pearsonr(data['empathy'], data['distress'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f296d90-1622-49a2-b27e-98923fdb1d0a",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd9005df-3ba4-414a-8d89-533e1a466ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.201\n",
      "pearson_r: 0.568\n",
      "pearson_r: 0.625\n",
      "pearson_r: 0.716\n",
      "pearson_r: 0.733\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.2\n",
      "pearson_r: 0.575\n",
      "pearson_r: 0.609\n",
      "pearson_r: 0.682\n",
      "pearson_r: 0.714\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.23\n",
      "pearson_r: 0.499\n",
      "pearson_r: 0.602\n",
      "pearson_r: 0.696\n",
      "pearson_r: 0.711\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.271\n",
      "pearson_r: 0.447\n",
      "pearson_r: 0.604\n",
      "pearson_r: 0.693\n",
      "pearson_r: 0.716\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.261\n",
      "pearson_r: 0.316\n",
      "pearson_r: 0.441\n",
      "pearson_r: 0.525\n",
      "pearson_r: 0.539\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.242\n",
      "pearson_r: 0.216\n",
      "pearson_r: 0.436\n",
      "pearson_r: 0.515\n",
      "pearson_r: 0.518\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.166\n",
      "pearson_r: 0.257\n",
      "pearson_r: 0.457\n",
      "pearson_r: 0.495\n",
      "pearson_r: 0.499\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.182\n",
      "pearson_r: 0.318\n",
      "pearson_r: 0.439\n",
      "pearson_r: 0.507\n",
      "pearson_r: 0.516\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.14\n",
      "pearson_r: 0.294\n",
      "pearson_r: 0.425\n",
      "pearson_r: 0.503\n",
      "pearson_r: 0.516\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.087\n",
      "pearson_r: 0.382\n",
      "pearson_r: 0.482\n",
      "pearson_r: 0.502\n",
      "pearson_r: 0.511\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.239\n",
      "pearson_r: 0.432\n",
      "pearson_r: 0.473\n",
      "pearson_r: 0.493\n",
      "pearson_r: 0.501\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.162\n",
      "pearson_r: 0.312\n",
      "pearson_r: 0.385\n",
      "pearson_r: 0.424\n",
      "pearson_r: 0.432\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7ce25289454bea81aa7fa3f4a96a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.139\n",
      "pearson_r: 0.166\n",
      "pearson_r: 0.189\n",
      "pearson_r: 0.2\n",
      "pearson_r: 0.205\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e7d1f-0c6b-48e0-8ba6-0f5b6d9526e0",
   "metadata": {},
   "source": [
    "## AutoModel vs AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2800df5-a062-4b38-a1e3-ac3ed77ab80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "75044741-ddb0-49e8-8c85-294dbec57f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTransformer(\n",
      "  (transformer): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (3): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "temp = CustomTransformer()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bebe2e60-ee9c-444a-aa67-d7fb344d2f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTransformerSequenceClassificaiton(\n",
      "  (transformer): DebertaV2ForSequenceClassification(\n",
      "    (deberta): DebertaV2Model(\n",
      "      (embeddings): DebertaV2Embeddings(\n",
      "        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "      (encoder): DebertaV2Encoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x DebertaV2Layer(\n",
      "            (attention): DebertaV2Attention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): DebertaV2SelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): DebertaV2Intermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DebertaV2Output(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (rel_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (pooler): ContextPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linearn): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "temp2 = CustomTransformerSequenceClassificaiton()\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9b9539de-c44c-40bc-a5be-8bb651aa2718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "CustomTransformer                                                 --\n",
       "DebertaV2Model: 1-1                                             --\n",
       "    DebertaV2Embeddings: 2-1                                   --\n",
       "        Embedding: 3-1                                        98,380,800\n",
       "        LayerNorm: 3-2                                        1,536\n",
       "        StableDropout: 3-3                                    --\n",
       "    DebertaV2Encoder: 2-2                                      --\n",
       "        ModuleList: 3-4                                       85,054,464\n",
       "        Embedding: 3-5                                        393,216\n",
       "        LayerNorm: 3-6                                        1,536\n",
       "Sequential: 1-2                                                 --\n",
       "    Linear: 2-3                                                590,592\n",
       "    Tanh: 2-4                                                  --\n",
       "    Linear: 2-5                                                590,592\n",
       "    Linear: 2-6                                                196,864\n",
       "    Linear: 2-7                                                257\n",
       "Dropout: 1-3                                                    --\n",
       "==========================================================================================\n",
       "Total params: 185,209,857\n",
       "Trainable params: 185,209,857\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d65a3053-5bbd-459a-a560-2548df8b83ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "CustomTransformerSequenceClassificaiton                                --\n",
       "DebertaV2ForSequenceClassification: 1-1                              --\n",
       "    DebertaV2Model: 2-1                                             --\n",
       "        DebertaV2Embeddings: 3-1                                   98,382,336\n",
       "        DebertaV2Encoder: 3-2                                      85,449,216\n",
       "    ContextPooler: 2-2                                              --\n",
       "        Linear: 3-3                                                590,592\n",
       "        StableDropout: 3-4                                         --\n",
       "    Linear: 2-3                                                     590,592\n",
       "    StableDropout: 2-4                                              --\n",
       "Dropout: 1-2                                                         --\n",
       "Linear: 1-3                                                          196,864\n",
       "Linear: 1-4                                                          257\n",
       "===============================================================================================\n",
       "Total params: 185,209,857\n",
       "Trainable params: 185,209,857\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2424bf-56e6-43df-83fe-e50e8b578fb5",
   "metadata": {},
   "source": [
    "## Earlier w/o *ForSequenceClassfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce85ff2e-a5a0-480e-9bea-42dcf4a22425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", num_labels=1):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        \n",
    "        self.num_labels=num_labels\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            ),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear1 = nn.Linear(768, 768)\n",
    "        self.linear2 = nn.Linear(768, 768)\n",
    "        self.linear3 = nn.Linear(768, 256)\n",
    "        self.linearn = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        \n",
    "        sequence_output = self.dropout(outputs[0]) #last_hidden_state\n",
    "        linear1_output = self.linear1(sequence_output[:,0,:].view(-1, 768)) #first token's embedding\n",
    "        linear2_output = self.linear2(F.tanh(linear1_output))\n",
    "        linear3_output = self.linear3(self.dropout(linear2_output))\n",
    "        logits = self.linearn(linear3_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6d3eb6-1029-41cb-b4f7-e639ce3ecf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(task=task, lr=1e-5, batch_size=8, n_epochs=3, n_freeze=0, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = CustomTransformer()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Freezing layers\n",
    "    if n_freeze:\n",
    "        # freeze the embedding layer when n_freeze = -1)\n",
    "        for param in model.transformer.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # freeze other layers as per n_freeze\n",
    "        if n_freeze != -1:\n",
    "            for layer in model.transformer.encoder.layer[:n_freeze]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    # print(\"Unfreezed layers:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainset = load_tokenised_data(\n",
    "        filename=train_file, task=task, tokenise_fn=tokenise, train=True\n",
    "    )\n",
    "           \n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # evaluation data loader in train mode to access labels\n",
    "    testset = load_tokenised_data(\n",
    "        filename=test_file, task=task, tokenise_fn=tokenise, train=True\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    training_steps = n_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(0, n_epochs):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch in testloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            y_true.extend((batch['labels'].tolist()))\n",
    "\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3e3295-42b9-4b56-b1b9-4d5aaf79ce41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.109\n",
      "pearson_r: 0.296\n",
      "pearson_r: 0.519\n",
      "pearson_r: 0.572\n",
      "pearson_r: 0.614\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.11\n",
      "pearson_r: 0.296\n",
      "pearson_r: 0.531\n",
      "pearson_r: 0.574\n",
      "pearson_r: 0.614\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.102\n",
      "pearson_r: 0.22\n",
      "pearson_r: 0.044\n",
      "pearson_r: 0.458\n",
      "pearson_r: 0.515\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.082\n",
      "pearson_r: 0.255\n",
      "pearson_r: 0.175\n",
      "pearson_r: 0.471\n",
      "pearson_r: 0.518\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.096\n",
      "pearson_r: 0.211\n",
      "pearson_r: 0.413\n",
      "pearson_r: 0.477\n",
      "pearson_r: 0.512\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.089\n",
      "pearson_r: 0.244\n",
      "pearson_r: 0.478\n",
      "pearson_r: 0.489\n",
      "pearson_r: 0.554\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.089\n",
      "pearson_r: 0.329\n",
      "pearson_r: 0.476\n",
      "pearson_r: 0.559\n",
      "pearson_r: 0.585\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.109\n",
      "pearson_r: 0.421\n",
      "pearson_r: 0.488\n",
      "pearson_r: 0.553\n",
      "pearson_r: 0.562\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.003\n",
      "pearson_r: 0.412\n",
      "pearson_r: 0.487\n",
      "pearson_r: 0.55\n",
      "pearson_r: 0.564\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.117\n",
      "pearson_r: 0.446\n",
      "pearson_r: 0.513\n",
      "pearson_r: 0.554\n",
      "pearson_r: 0.561\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.197\n",
      "pearson_r: 0.448\n",
      "pearson_r: 0.529\n",
      "pearson_r: 0.566\n",
      "pearson_r: 0.574\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.06\n",
      "pearson_r: 0.256\n",
      "pearson_r: 0.355\n",
      "pearson_r: 0.401\n",
      "pearson_r: 0.413\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f43f782100f45d6935ceece35239214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ba2907b3c148308ea336f30315a597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.155\n",
      "pearson_r: 0.177\n",
      "pearson_r: 0.203\n",
      "pearson_r: 0.207\n",
      "pearson_r: 0.215\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97529c70-6607-4447-8c2d-06bb551dffcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.243\n",
      "pearson_r: 0.449\n",
      "pearson_r: 0.51\n",
      "pearson_r: 0.553\n",
      "pearson_r: 0.57\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.246\n",
      "pearson_r: 0.454\n",
      "pearson_r: 0.51\n",
      "pearson_r: 0.558\n",
      "pearson_r: 0.575\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.405\n",
      "pearson_r: 0.48\n",
      "pearson_r: 0.505\n",
      "pearson_r: 0.57\n",
      "pearson_r: 0.585\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.398\n",
      "pearson_r: 0.455\n",
      "pearson_r: 0.551\n",
      "pearson_r: 0.612\n",
      "pearson_r: 0.625\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.258\n",
      "pearson_r: 0.415\n",
      "pearson_r: 0.531\n",
      "pearson_r: 0.589\n",
      "pearson_r: 0.592\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.261\n",
      "pearson_r: 0.446\n",
      "pearson_r: 0.494\n",
      "pearson_r: 0.541\n",
      "pearson_r: 0.582\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.212\n",
      "pearson_r: 0.423\n",
      "pearson_r: 0.45\n",
      "pearson_r: 0.493\n",
      "pearson_r: 0.524\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.307\n",
      "pearson_r: 0.445\n",
      "pearson_r: 0.477\n",
      "pearson_r: 0.533\n",
      "pearson_r: 0.554\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.22\n",
      "pearson_r: 0.41\n",
      "pearson_r: 0.459\n",
      "pearson_r: 0.475\n",
      "pearson_r: 0.485\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.195\n",
      "pearson_r: 0.369\n",
      "pearson_r: 0.436\n",
      "pearson_r: 0.473\n",
      "pearson_r: 0.483\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.062\n",
      "pearson_r: 0.36\n",
      "pearson_r: 0.458\n",
      "pearson_r: 0.494\n",
      "pearson_r: 0.504\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.127\n",
      "pearson_r: 0.268\n",
      "pearson_r: 0.341\n",
      "pearson_r: 0.383\n",
      "pearson_r: 0.394\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99d1b7e4004df69e6cf98e1cc802b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f77ca0147464c5ab3e4645ddf556fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.148\n",
      "pearson_r: 0.134\n",
      "pearson_r: 0.158\n",
      "pearson_r: 0.174\n",
      "pearson_r: 0.179\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Earlier without pooling effort\n",
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
