{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d78693-aff6-4484-9689-b92b91844b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_scheduler\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd96ca12-af79-4a12-8f47-218e424f8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"./text-empathy/\")\n",
    "from evaluation import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561a3d26-7b1e-4458-8dd8-177ee1c49187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, y2=None, xlabel=None, ylabel=None, legend=[], save=False, filename=None):\n",
    "    \"\"\"Plot data points\"\"\"\n",
    "    plt.style.use(['science'])\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    \n",
    "    ax.plot(x,y)\n",
    "    if y2 is not None:\n",
    "        ax.plot(x, y2)\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(legend)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(fname=filename+'.pdf', format = 'pdf', bbox_inches='tight')\n",
    "        print(f\"Saved as {filename}.pdf\")\n",
    "        \n",
    "    fig.show()\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "623f0a4d-9e54-42e5-bfe3-4c80f728a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['empathy', 'distress']\n",
    "\n",
    "checkpoint = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "train_file = \"./data/PREPROCESSED-essay-train.csv\"\n",
    "dev_file = \"./data/PREPROCESSED-essay-dev.csv\"\n",
    "train_dev_file = \"./data/PREPROCESSED-essay-train-dev.csv\"\n",
    "test_file = \"./data/PREPROCESSED-test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba39ac-c3c1-47be-98ae-c2752c8ff953",
   "metadata": {},
   "source": [
    "# Multi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0f263859-7a93-4520-be66-8e0ed3424847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, task, feature_to_tokenise, train, batch_size, shuffle):\n",
    "\n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_to_tokenise], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[[feature_to_tokenise, task]]\n",
    "    else:\n",
    "        chosen_data = input_data[[feature_to_tokenise]]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "    \n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = [feature_to_tokenise])\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task, \"labels\") # as huggingface requires\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "           \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "451d2b22-d74f-4be3-89c4-76f04b5f5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(task, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    trainloader, devloader = [], []\n",
    "    for feature in features:\n",
    "        trainloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=train_file, task=task, feature_to_tokenise=feature, train=True, batch_size=batch_size, shuffle=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # devloader in train mode to pass labels\n",
    "        devloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=dev_file, task=task, feature_to_tokenise=feature, train=True, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return trainloader, devloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "46b1bb74-49bf-4a64-ba41-3c3c3acecaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(task, features, batch_size, mode):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    testloader = []\n",
    "    for feature in features:\n",
    "        testloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=test_file, task=task, feature_to_tokenise=feature, train=False, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb1835-6b60-4fe0-90da-ad8f2d7b1dc4",
   "metadata": {},
   "source": [
    "## Tri-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5e5bcde6-6710-4da2-9370-472eb77b1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['demographic', 'essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f17afdd1-eb47-49f4-add8-a9da34160099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(TriEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer_f1 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.transformer_f2 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.transformer_f3 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(2304, 768) #768+768+768 = 2304 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        self.fc4 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer_f1.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.transformer_f2.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.transformer_f3.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer_f1.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                for layer in self.transformer_f2.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                for layer in self.transformer_f3.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_f1=None,\n",
    "        attention_mask_f1=None,\n",
    "        input_ids_f2=None,\n",
    "        attention_mask_f2=None,\n",
    "        input_ids_f3=None,\n",
    "        attention_mask_f3=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs_f1 = self.transformer_f1(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer_f1(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        outputs_f3 = self.transformer_f1(\n",
    "            input_ids = input_ids_f3,\n",
    "            attention_mask = attention_mask_f3,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "        outputs_f3_last_state = outputs_f3[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "        outputs_f3_cls = outputs_f3_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls, outputs_f3_cls), dim=1) # shape: (batch_size, 768*3)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        logits = self.fc4(X)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            # hidden_states=outputs.hidden_states,\n",
    "            # attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3fa2b668-040e-481e-b3b9-24a0c7fcdda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = TriEncoderTransformer()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(task=task, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader[0])\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2, batch_f3) in zip(trainloader[0], trainloader[1], trainloader[2]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "                'attention_mask_f3': batch_f3['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader[0])) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2, batch_f3) in zip(devloader[0], devloader[1], devloader[2]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "                'attention_mask_f3': batch_f3['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device)\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            y_true.extend((batch_f1['labels'].tolist())) #batch_f2 labels should be the same\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader[0]))\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "    \n",
    "    # train-test finished\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "    print(\"Saved the model as model.pth\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c48b7e-12f4-4feb-9061-4bcf217c2895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.106\n",
      "pearson_r: 0.223\n",
      "pearson_r: 0.299\n",
      "pearson_r: 0.626\n",
      "pearson_r: 0.459\n",
      "pearson_r: 0.521\n",
      "pearson_r: 0.556\n",
      "pearson_r: 0.625\n",
      "pearson_r: 0.707\n",
      "pearson_r: 0.647\n",
      "pearson_r: 0.73\n",
      "pearson_r: 0.716\n",
      "pearson_r: 0.704\n",
      "pearson_r: 0.702\n"
     ]
    }
   ],
   "source": [
    "train(n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b4a4ff95-0bd8-41b4-8720-a90cf4337790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(task=tasks[0], batch_size=8):\n",
    "    \n",
    "    model = TriEncoderTransformer()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # load the trained parameters\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    \n",
    "    testloader = get_test_data(task=task, features=features, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    for (batch_f1, batch_f2, batch_f3) in zip(testloader[0], testloader[1], testloader[2]):\n",
    "        batch = {\n",
    "            'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "            'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "            'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "            'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "            'input_ids_f3': batch_f3['input_ids'].to(device),\n",
    "            'attention_mask_f3': batch_f3['attention_mask'].to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "        y_pred.extend(batch_pred)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e15271e5-184d-4b46-aec7-a9e269c0841a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = test(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d5e6b-55ed-42f1-aee7-c14625b0202e",
   "metadata": {},
   "source": [
    "## Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1d6a54f-2f10-40cc-9099-119fe64c6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['demographic_essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2affe2e-f389-4d0c-996a-782b410da1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(BiEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer_f1 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.transformer_f2 = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(1536, 768) #768+768 = 1536 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        self.fc4 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer_f1.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.transformer_f2.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer_f1.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                for layer in self.transformer_f2.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids_f1=None, attention_mask_f1=None, input_ids_f2=None, attention_mask_f2=None, labels=None):\n",
    "        outputs_f1 = self.transformer_f1(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer_f2(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls), dim=1) # shape: (batch_size, 1536)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        logits = self.fc4(X)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            # hidden_states=outputs.hidden_states,\n",
    "            # attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2debfb14-e1d6-4984-86ef-cc74781ff571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = BiEncoderTransformer()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(task=task, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader[0])\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2) in zip(trainloader[0], trainloader[1]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader[0])) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2) in zip(devloader[0], devloader[1]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels': batch_f1['labels'].to(device)\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            y_true.extend((batch_f1['labels'].tolist())) #batch_f2 labels should be the same\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader[0]))\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "    \n",
    "    # train-test finished\n",
    "    # torch.save(model.state_dict(), \"model.pth\")\n",
    "    # print(\"Saved the model as model.pth\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f0dba19-dc03-42a2-8243-b3e90c89b1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.175\n",
      "pearson_r: 0.485\n",
      "pearson_r: 0.476\n",
      "pearson_r: 0.55\n",
      "pearson_r: 0.549\n",
      "pearson_r: 0.586\n",
      "pearson_r: 0.615\n",
      "pearson_r: 0.61\n",
      "pearson_r: 0.602\n",
      "pearson_r: 0.599\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=10, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa907ad5-bebc-4870-9887-66d3cb1efdf4",
   "metadata": {},
   "source": [
    "# Single encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2c13cbfe-6390-49e7-9bca-a147a48e7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['demographic_essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "534441d0-6378-479d-b82f-af1f3abe1316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, task, feature_to_tokenise, train, batch_size, shuffle):\n",
    "    feature_1 = feature_to_tokenise[0]\n",
    "    feature_2 = feature_to_tokenise[1]\n",
    "    \n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_1], sentence[feature_2], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[feature_to_tokenise + [task]]\n",
    "    else:\n",
    "        chosen_data = input_data[feature_to_tokenise]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = feature_to_tokenise)\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task, \"labels\") # as huggingface requires\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "    \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c97b1bea-0424-437d-a15f-5c10a2fe3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(task, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    trainloader = load_tokenised_data(\n",
    "        filename=train_file, task=task, feature_to_tokenise=features, train=True, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # devloader in train mode to pass labels\n",
    "    devloader = load_tokenised_data(\n",
    "        filename=dev_file, task=task, feature_to_tokenise=features, train=True, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return trainloader, devloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ca1bc714-7064-4c74-b4eb-35a4bf9f2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(task, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    testloader = load_tokenised_data(\n",
    "        filename=test_file, task=task, feature_to_tokenise=feature, train=False, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e53f604b-0e21-4091-822c-24165e9fc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(CustomRegressor, self).__init__()\n",
    "\n",
    "        self.transformer_last_hidden_size = 768\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "                num_labels=self.transformer_last_hidden_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(self.transformer_last_hidden_size, 256)\n",
    "        # self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.deberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.deberta.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        # print(outputs.keys())\n",
    "        # will return ['logits', 'hidden_states', 'attentions']\n",
    "\n",
    "        x = self.dropout(outputs.logits) # output.logits aka pooled output from [CLS] token beacuse of AutoModelForSequenceClassification\n",
    "        # x = F.tanh(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "                \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ac1d482d-a1b9-450a-83fa-586a7eaeec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = CustomRegressor()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # print(model)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "              \n",
    "    trainloader, devloader = get_data(task=tasks[0], features=features, batch_size=batch_size)\n",
    "\n",
    "    training_steps = n_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss.append(total_loss / len(trainloader)) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch in devloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "            y_true.extend((batch['labels'].tolist()))\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader))\n",
    "\n",
    "        # print(y_pred)\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cc3c150-e51b-4501-9d47-b05057cf2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.324\n",
      "pearson_r: 0.507\n",
      "pearson_r: 0.689\n",
      "pearson_r: 0.653\n",
      "pearson_r: 0.71\n",
      "pearson_r: 0.711\n",
      "pearson_r: 0.705\n",
      "pearson_r: 0.703\n",
      "pearson_r: 0.697\n",
      "pearson_r: 0.709\n",
      "pearson_r: 0.71\n",
      "pearson_r: 0.71\n",
      "pearson_r: 0.698\n",
      "pearson_r: 0.687\n",
      "pearson_r: 0.701\n",
      "pearson_r: 0.699\n",
      "pearson_r: 0.699\n",
      "pearson_r: 0.7\n",
      "pearson_r: 0.699\n",
      "pearson_r: 0.696\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=20, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5d5a1-2ab5-47ec-877a-f931fde5c110",
   "metadata": {},
   "source": [
    "# Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8ba67-10ae-4d31-aa58-c2e571b0c1f1",
   "metadata": {},
   "source": [
    "## Bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "38e29437-8b87-46af-b698-ae5d431c267e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, tasks, feature_to_tokenise, train, batch_size, shuffle):\n",
    "\n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_to_tokenise], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[[feature_to_tokenise] + tasks]\n",
    "    else:\n",
    "        chosen_data = input_data[[feature_to_tokenise]]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = [feature_to_tokenise])\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(tasks[0], \"labels_1\") # more meaningful\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(tasks[1], \"labels_2\")\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "           \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d67986a0-05db-4bed-88c8-6156a73ee9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tasks, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    trainloader, devloader = [], []\n",
    "    for feature in features:\n",
    "        trainloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=train_file, tasks=tasks, feature_to_tokenise=feature, train=True, batch_size=batch_size, shuffle=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # devloader in train mode to pass labels\n",
    "        devloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=dev_file, tasks=tasks, feature_to_tokenise=feature, train=True, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return trainloader, devloader\n",
    "\n",
    "def get_test_data(tasks, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    testloader = []\n",
    "    for feature in features:\n",
    "        testloader.append(\n",
    "            load_tokenised_data(\n",
    "                filename=test_file, tasks=tasks, feature_to_tokenise=feature, train=False, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "256b6524-9ef8-440d-9a32-8f8803166c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(BiEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(1536, 768) #768+768 = 1536 as I concatenate the sequence outputs\n",
    "        self.fc2 = nn.Linear(768, 768)\n",
    "        self.fc3 = nn.Linear(768, 256)\n",
    "        \n",
    "        self.fc4_1 = nn.Linear(256, 1) # regression problem\n",
    "        self.fc4_2 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_f1=None,\n",
    "        attention_mask_f1=None,\n",
    "        input_ids_f2=None,\n",
    "        attention_mask_f2=None,\n",
    "        labels_1=None,\n",
    "        labels_2=None\n",
    "    ):\n",
    "        outputs_f1 = self.transformer(\n",
    "            input_ids = input_ids_f1,\n",
    "            attention_mask = attention_mask_f1,\n",
    "        )\n",
    "\n",
    "        outputs_f2 = self.transformer(\n",
    "            input_ids = input_ids_f2,\n",
    "            attention_mask = attention_mask_f2,\n",
    "        )\n",
    "\n",
    "        # print(outputs_f1.keys()) #result: odict_keys(['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "        outputs_f1_last_state = outputs_f1[0]\n",
    "        outputs_f2_last_state = outputs_f2[0]\n",
    "       \n",
    "        outputs_f1_cls = outputs_f1_last_state[:,0,:].view(-1, 768) # shape: (batch_size, 768)\n",
    "        outputs_f2_cls = outputs_f2_last_state[:,0,:].view(-1, 768)\n",
    "\n",
    "        X = torch.cat((outputs_f1_cls, outputs_f2_cls), dim=1) # shape: (batch_size, 1536)\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        X = F.tanh(self.fc2(X))\n",
    "        X = F.tanh(self.fc3(X))\n",
    "        \n",
    "        logits_1 = self.fc4_1(X)\n",
    "        logits_2 = self.fc4_2(X)\n",
    "        \n",
    "        loss_1 = None\n",
    "        if labels_1 is not None:\n",
    "            loss_1 = F.mse_loss(logits_1.view(-1), labels_1.view(-1))\n",
    "\n",
    "        loss_2 = None\n",
    "        if labels_2 is not None:\n",
    "            loss_2 = F.mse_loss(logits_2.view(-1), labels_2.view(-1))\n",
    "        \n",
    "        return (\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_1,\n",
    "                logits=logits_1\n",
    "            ),\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_2,\n",
    "                logits=logits_2\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e1fb28b-edc0-4a29-a3a0-138386504c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = BiEncoderTransformer()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(tasks=tasks, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader[0])\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    wgt_1=1\n",
    "    wgt_2=1\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for (batch_f1, batch_f2) in zip(trainloader[0], trainloader[1]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels_1': batch_f1['labels_1'].to(device), #batch_f2 labels should be the same\n",
    "                'labels_2': batch_f1['labels_2'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            opt.zero_grad()\n",
    "            (outputs_1, outputs_2) = model(**batch)\n",
    "            loss_1 = outputs_1.loss\n",
    "            loss_2 = outputs_2.loss\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader[0])) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for (batch_f1, batch_f2) in zip(devloader[0], devloader[1]):\n",
    "            batch = {\n",
    "                'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "                'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "                'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "                'attention_mask_f2': batch_f2['attention_mask'].to(device),\n",
    "                'labels_1': batch_f1['labels_1'].to(device), #batch_f2 labels should be the same\n",
    "                'labels_2': batch_f1['labels_2'].to(device) #batch_f2 labels should be the same\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                (outputs_1, outputs_2) = model(**batch)\n",
    "\n",
    "            batch_pred_1 = [item for sublist in outputs_1.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred_1)\n",
    "\n",
    "            y_true.extend((batch_f1['labels_1'].tolist())) #batch_f2 labels should be the same\n",
    "\n",
    "            loss_1 = outputs_1.loss.item()\n",
    "            loss_2 = outputs_2.loss.item()\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "\n",
    "            total_loss += loss\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader[0]))\n",
    "        \n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    # model_save_as = \"model-biencoder-mtl.pth\"\n",
    "    # torch.save(model.state_dict(), model_save_as)\n",
    "    # print(f\"Saved the model as {model_save_as}\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f839da50-71b7-4842-b2da-1a658f5081b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.043\n",
      "pearson_r: 0.072\n",
      "pearson_r: 0.196\n",
      "pearson_r: 0.301\n",
      "pearson_r: 0.479\n",
      "pearson_r: 0.525\n",
      "pearson_r: 0.576\n",
      "pearson_r: 0.608\n",
      "pearson_r: 0.661\n",
      "pearson_r: 0.681\n",
      "pearson_r: 0.65\n",
      "pearson_r: 0.657\n",
      "pearson_r: 0.655\n",
      "pearson_r: 0.666\n",
      "pearson_r: 0.671\n",
      "pearson_r: 0.68\n",
      "pearson_r: 0.682\n",
      "pearson_r: 0.677\n",
      "pearson_r: 0.679\n",
      "pearson_r: 0.687\n",
      "pearson_r: 0.69\n",
      "pearson_r: 0.7\n",
      "pearson_r: 0.695\n",
      "pearson_r: 0.693\n",
      "pearson_r: 0.684\n",
      "pearson_r: 0.696\n",
      "pearson_r: 0.68\n",
      "pearson_r: 0.693\n",
      "pearson_r: 0.701\n",
      "pearson_r: 0.697\n",
      "pearson_r: 0.706\n",
      "pearson_r: 0.682\n",
      "pearson_r: 0.678\n",
      "pearson_r: 0.702\n",
      "pearson_r: 0.714\n",
      "pearson_r: 0.705\n",
      "pearson_r: 0.709\n",
      "pearson_r: 0.715\n",
      "pearson_r: 0.715\n",
      "pearson_r: 0.708\n",
      "pearson_r: 0.71\n",
      "pearson_r: 0.7\n",
      "pearson_r: 0.706\n",
      "pearson_r: 0.706\n",
      "pearson_r: 0.706\n",
      "pearson_r: 0.694\n",
      "pearson_r: 0.703\n",
      "pearson_r: 0.703\n",
      "pearson_r: 0.706\n",
      "pearson_r: 0.703\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(lr=1e-5, batch_size=8, n_epochs=50, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c34a68ef-39ed-4520-9e36-aa1c46e94839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(batch_size=8):\n",
    "    \n",
    "    model = BiEncoderTransformer()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    # load the trained parameters\n",
    "    model.load_state_dict(torch.load(\"model-biencoder-mtl.pth\"))\n",
    "    \n",
    "    testloader = get_test_data(tasks=tasks, features=features, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred_1, y_pred_2 = [], []\n",
    "    for (batch_f1, batch_f2) in zip(testloader[0], testloader[1]):\n",
    "        batch = {\n",
    "            'input_ids_f1': batch_f1['input_ids'].to(device),\n",
    "            'attention_mask_f1': batch_f1['attention_mask'].to(device),\n",
    "            'input_ids_f2': batch_f2['input_ids'].to(device),\n",
    "            'attention_mask_f2': batch_f2['attention_mask'].to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            (outputs_1, outputs_2) = model(**batch)\n",
    "\n",
    "        batch_pred_1 = [item for sublist in outputs_1.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "        y_pred_1.extend(batch_pred_1)\n",
    "        batch_pred_2 = [item for sublist in outputs_2.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "        y_pred_2.extend(batch_pred_2)\n",
    "\n",
    "    # for submission to CodaLab\n",
    "    y_pred = pd.DataFrame({\n",
    "        tasks[0]: y_pred_1,\n",
    "        tasks[1]: y_pred_2\n",
    "    })\n",
    "    y_pred.to_csv(\"./tmp/predictions_EMP.tsv\", sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3156124b-752e-4a85-8353-b2e3698afe2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819166a-84fa-49a7-b09e-2a1f12cae567",
   "metadata": {},
   "source": [
    "## Single encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0990b1bb-2e93-4d12-bc68-21e2e579be60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, tasks, feature_to_tokenise, train, batch_size, shuffle):\n",
    "    feature_1 = feature_to_tokenise[0]\n",
    "    feature_2 = feature_to_tokenise[1]\n",
    "    \n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_1], sentence[feature_2], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[feature_to_tokenise + tasks] # + means list concat\n",
    "    else:\n",
    "        chosen_data = input_data[feature_to_tokenise]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = feature_to_tokenise)\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(tasks[0], \"labels_1\") # more meaningful\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(tasks[1], \"labels_2\")\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "           \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "24bb2661-c129-447c-81f6-632f2d63bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tasks, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    trainloader = load_tokenised_data(\n",
    "        filename=train_file, tasks=tasks, feature_to_tokenise=features, train=True, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "        # devloader in train mode to pass labels\n",
    "    devloader = load_tokenised_data(\n",
    "        filename=dev_file, tasks=tasks, feature_to_tokenise=features, train=True, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return trainloader, devloader\n",
    "\n",
    "def get_test_data(tasks, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    testloader = load_tokenised_data(\n",
    "        filename=test_file, tasks=tasks, feature_to_tokenise=features, train=False, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e33298f7-bf9a-4928-8502-0cc662ce2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleEncoderTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(SingleEncoderTransformer, self).__init__()\n",
    "        \n",
    "        self.transformer_last_hidden_size = 768\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "                num_labels=self.transformer_last_hidden_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.fc1 = nn.Linear(self.transformer_last_hidden_size, 256)\n",
    "        # self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3_1 = nn.Linear(256, 1) # regression problem\n",
    "        self.fc3_2 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.deberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.deberta.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels_1=None,\n",
    "        labels_2=None\n",
    "    ):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "\n",
    "        # print(outputs.keys())\n",
    "        # will return ['logits', 'hidden_states', 'attentions']\n",
    "        \n",
    "        X = self.dropout(outputs.logits)\n",
    "        X = F.tanh(self.fc1(X))\n",
    "        # X = F.tanh(self.fc2(X))\n",
    "        \n",
    "        logits_1 = self.fc3_1(X)\n",
    "        logits_2 = self.fc3_2(X)\n",
    "        \n",
    "        loss_1 = None\n",
    "        if labels_1 is not None:\n",
    "            loss_1 = F.mse_loss(logits_1.view(-1), labels_1.view(-1))\n",
    "\n",
    "        loss_2 = None\n",
    "        if labels_2 is not None:\n",
    "            loss_2 = F.mse_loss(logits_2.view(-1), labels_2.view(-1))\n",
    "        \n",
    "        return (\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_1,\n",
    "                logits=logits_1\n",
    "            ),\n",
    "            SequenceClassifierOutput(\n",
    "                loss=loss_2,\n",
    "                logits=logits_2\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2a87e4bc-57bf-4ab8-b916-21229b493915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=3, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = SingleEncoderTransformer()\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainloader, devloader = get_data(tasks=tasks, features=features, batch_size=batch_size)\n",
    "    \n",
    "    training_steps = n_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    wgt_1=1\n",
    "    wgt_2=1\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            opt.zero_grad()\n",
    "            (outputs_1, outputs_2) = model(**batch)\n",
    "            loss_1 = outputs_1.loss\n",
    "            loss_2 = outputs_2.loss\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(trainloader)) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch in devloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                (outputs_1, outputs_2) = model(**batch)\n",
    "\n",
    "            batch_pred_1 = [item for sublist in outputs_1.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred_1)\n",
    "\n",
    "            y_true.extend((batch['labels_1'].tolist())) #batch_f2 labels should be the same\n",
    "\n",
    "            loss_1 = outputs_1.loss.item()\n",
    "            loss_2 = outputs_2.loss.item()\n",
    "            loss = wgt_1*loss_1 + wgt_2*loss_2\n",
    "\n",
    "            total_loss += loss\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader))\n",
    "        \n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))\n",
    "\n",
    "    # train-test finished\n",
    "    # model_save_as = \"model-biencoder-mtl.pth\"\n",
    "    # torch.save(model.state_dict(), model_save_as)\n",
    "    # print(f\"Saved the model as {model_save_as}\")\n",
    "    \n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "00cd9433-8c47-41c6-a636-24448af99db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezed layers: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.141\n",
      "pearson_r: 0.406\n",
      "pearson_r: 0.522\n",
      "pearson_r: 0.627\n",
      "pearson_r: 0.603\n",
      "pearson_r: 0.664\n",
      "pearson_r: 0.677\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[136], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(task, lr, batch_size, n_epochs, seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 47\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)) \u001b[38;5;66;03m# len(trainloader) == num_batches\u001b[39;00m\n\u001b[1;32m     50\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=15, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d019e5-0361-4b43-a6e2-7a45c18a7b65",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d2adecf5-6ebf-4b2b-8ca9-5820ec461360",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = dev_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d25e607c-d7e9-4cf6-a9a0-e9d7da121fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_1 = pd.read_csv(\"./data/WASSA23/goldstandard_dev.tsv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6562dfc2-b47a-47c8-88a8-af54bc9fece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = pd.read_csv(\"./tmp/predictions_EMP.tsv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "caf34bbf-c558-437a-a646-dfdbc77fb261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(true_1.iloc[:, 0], pred_1.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa154a-d5b7-46c4-a85b-933f2cc38ded",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1be1e38-d79d-4f37-bc00-6bf3ce2983c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(train_file, header=0, index_col=0)\n",
    "pearsonr(data['empathy'], data['distress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d410c466-ce0b-45e5-83f2-7745af9e109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.594"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(test_file, header=0, index_col=0)\n",
    "pearsonr(data['empathy'], data['distress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3317864c-a416-4603-b8bc-d9119a0c8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.read_csv(\"./tmp/predictions_EMP.tsv\", sep='\\t', header=None, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "30ce2323-c15e-429d-b060-2b49c32807a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.906"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(y_pred.iloc[:,0], y_pred.iloc[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f296d90-1622-49a2-b27e-98923fdb1d0a",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd9005df-3ba4-414a-8d89-533e1a466ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.201\n",
      "pearson_r: 0.568\n",
      "pearson_r: 0.625\n",
      "pearson_r: 0.716\n",
      "pearson_r: 0.733\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.2\n",
      "pearson_r: 0.575\n",
      "pearson_r: 0.609\n",
      "pearson_r: 0.682\n",
      "pearson_r: 0.714\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.23\n",
      "pearson_r: 0.499\n",
      "pearson_r: 0.602\n",
      "pearson_r: 0.696\n",
      "pearson_r: 0.711\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.271\n",
      "pearson_r: 0.447\n",
      "pearson_r: 0.604\n",
      "pearson_r: 0.693\n",
      "pearson_r: 0.716\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.261\n",
      "pearson_r: 0.316\n",
      "pearson_r: 0.441\n",
      "pearson_r: 0.525\n",
      "pearson_r: 0.539\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.242\n",
      "pearson_r: 0.216\n",
      "pearson_r: 0.436\n",
      "pearson_r: 0.515\n",
      "pearson_r: 0.518\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.166\n",
      "pearson_r: 0.257\n",
      "pearson_r: 0.457\n",
      "pearson_r: 0.495\n",
      "pearson_r: 0.499\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.182\n",
      "pearson_r: 0.318\n",
      "pearson_r: 0.439\n",
      "pearson_r: 0.507\n",
      "pearson_r: 0.516\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.14\n",
      "pearson_r: 0.294\n",
      "pearson_r: 0.425\n",
      "pearson_r: 0.503\n",
      "pearson_r: 0.516\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.087\n",
      "pearson_r: 0.382\n",
      "pearson_r: 0.482\n",
      "pearson_r: 0.502\n",
      "pearson_r: 0.511\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.239\n",
      "pearson_r: 0.432\n",
      "pearson_r: 0.473\n",
      "pearson_r: 0.493\n",
      "pearson_r: 0.501\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.162\n",
      "pearson_r: 0.312\n",
      "pearson_r: 0.385\n",
      "pearson_r: 0.424\n",
      "pearson_r: 0.432\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7ce25289454bea81aa7fa3f4a96a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.139\n",
      "pearson_r: 0.166\n",
      "pearson_r: 0.189\n",
      "pearson_r: 0.2\n",
      "pearson_r: 0.205\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e7d1f-0c6b-48e0-8ba6-0f5b6d9526e0",
   "metadata": {},
   "source": [
    "## AutoModel vs AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2800df5-a062-4b38-a1e3-ac3ed77ab80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "75044741-ddb0-49e8-8c85-294dbec57f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTransformer(\n",
      "  (transformer): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (3): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "temp = CustomTransformer()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bebe2e60-ee9c-444a-aa67-d7fb344d2f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTransformerSequenceClassificaiton(\n",
      "  (transformer): DebertaV2ForSequenceClassification(\n",
      "    (deberta): DebertaV2Model(\n",
      "      (embeddings): DebertaV2Embeddings(\n",
      "        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "      (encoder): DebertaV2Encoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x DebertaV2Layer(\n",
      "            (attention): DebertaV2Attention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): DebertaV2SelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): DebertaV2Intermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DebertaV2Output(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (rel_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (pooler): ContextPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linearn): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "temp2 = CustomTransformerSequenceClassificaiton()\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9b9539de-c44c-40bc-a5be-8bb651aa2718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "CustomTransformer                                                 --\n",
       "DebertaV2Model: 1-1                                             --\n",
       "    DebertaV2Embeddings: 2-1                                   --\n",
       "        Embedding: 3-1                                        98,380,800\n",
       "        LayerNorm: 3-2                                        1,536\n",
       "        StableDropout: 3-3                                    --\n",
       "    DebertaV2Encoder: 2-2                                      --\n",
       "        ModuleList: 3-4                                       85,054,464\n",
       "        Embedding: 3-5                                        393,216\n",
       "        LayerNorm: 3-6                                        1,536\n",
       "Sequential: 1-2                                                 --\n",
       "    Linear: 2-3                                                590,592\n",
       "    Tanh: 2-4                                                  --\n",
       "    Linear: 2-5                                                590,592\n",
       "    Linear: 2-6                                                196,864\n",
       "    Linear: 2-7                                                257\n",
       "Dropout: 1-3                                                    --\n",
       "==========================================================================================\n",
       "Total params: 185,209,857\n",
       "Trainable params: 185,209,857\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d65a3053-5bbd-459a-a560-2548df8b83ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "CustomTransformerSequenceClassificaiton                                --\n",
       "DebertaV2ForSequenceClassification: 1-1                              --\n",
       "    DebertaV2Model: 2-1                                             --\n",
       "        DebertaV2Embeddings: 3-1                                   98,382,336\n",
       "        DebertaV2Encoder: 3-2                                      85,449,216\n",
       "    ContextPooler: 2-2                                              --\n",
       "        Linear: 3-3                                                590,592\n",
       "        StableDropout: 3-4                                         --\n",
       "    Linear: 2-3                                                     590,592\n",
       "    StableDropout: 2-4                                              --\n",
       "Dropout: 1-2                                                         --\n",
       "Linear: 1-3                                                          196,864\n",
       "Linear: 1-4                                                          257\n",
       "===============================================================================================\n",
       "Total params: 185,209,857\n",
       "Trainable params: 185,209,857\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2424bf-56e6-43df-83fe-e50e8b578fb5",
   "metadata": {},
   "source": [
    "## Earlier w/o *ForSequenceClassfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce85ff2e-a5a0-480e-9bea-42dcf4a22425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", num_labels=1):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        \n",
    "        self.num_labels=num_labels\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True\n",
    "            ),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear1 = nn.Linear(768, 768)\n",
    "        self.linear2 = nn.Linear(768, 768)\n",
    "        self.linear3 = nn.Linear(768, 256)\n",
    "        self.linearn = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        \n",
    "        sequence_output = self.dropout(outputs[0]) #last_hidden_state\n",
    "        linear1_output = self.linear1(sequence_output[:,0,:].view(-1, 768)) #first token's embedding\n",
    "        linear2_output = self.linear2(F.tanh(linear1_output))\n",
    "        linear3_output = self.linear3(self.dropout(linear2_output))\n",
    "        logits = self.linearn(linear3_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6d3eb6-1029-41cb-b4f7-e639ce3ecf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(task=task, lr=1e-5, batch_size=8, n_epochs=3, n_freeze=0, seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = CustomTransformer()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Freezing layers\n",
    "    if n_freeze:\n",
    "        # freeze the embedding layer when n_freeze = -1)\n",
    "        for param in model.transformer.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # freeze other layers as per n_freeze\n",
    "        if n_freeze != -1:\n",
    "            for layer in model.transformer.encoder.layer[:n_freeze]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    # print(\"Unfreezed layers:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainset = load_tokenised_data(\n",
    "        filename=train_file, task=task, tokenise_fn=tokenise, train=True\n",
    "    )\n",
    "           \n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # evaluation data loader in train mode to access labels\n",
    "    testset = load_tokenised_data(\n",
    "        filename=test_file, task=task, tokenise_fn=tokenise, train=True\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    training_steps = n_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(0, n_epochs):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch in testloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "\n",
    "            y_true.extend((batch['labels'].tolist()))\n",
    "\n",
    "        print(\"pearson_r:\", pearsonr(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3e3295-42b9-4b56-b1b9-4d5aaf79ce41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.109\n",
      "pearson_r: 0.296\n",
      "pearson_r: 0.519\n",
      "pearson_r: 0.572\n",
      "pearson_r: 0.614\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.11\n",
      "pearson_r: 0.296\n",
      "pearson_r: 0.531\n",
      "pearson_r: 0.574\n",
      "pearson_r: 0.614\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.102\n",
      "pearson_r: 0.22\n",
      "pearson_r: 0.044\n",
      "pearson_r: 0.458\n",
      "pearson_r: 0.515\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.082\n",
      "pearson_r: 0.255\n",
      "pearson_r: 0.175\n",
      "pearson_r: 0.471\n",
      "pearson_r: 0.518\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.096\n",
      "pearson_r: 0.211\n",
      "pearson_r: 0.413\n",
      "pearson_r: 0.477\n",
      "pearson_r: 0.512\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.089\n",
      "pearson_r: 0.244\n",
      "pearson_r: 0.478\n",
      "pearson_r: 0.489\n",
      "pearson_r: 0.554\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.089\n",
      "pearson_r: 0.329\n",
      "pearson_r: 0.476\n",
      "pearson_r: 0.559\n",
      "pearson_r: 0.585\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.109\n",
      "pearson_r: 0.421\n",
      "pearson_r: 0.488\n",
      "pearson_r: 0.553\n",
      "pearson_r: 0.562\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.003\n",
      "pearson_r: 0.412\n",
      "pearson_r: 0.487\n",
      "pearson_r: 0.55\n",
      "pearson_r: 0.564\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.117\n",
      "pearson_r: 0.446\n",
      "pearson_r: 0.513\n",
      "pearson_r: 0.554\n",
      "pearson_r: 0.561\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.197\n",
      "pearson_r: 0.448\n",
      "pearson_r: 0.529\n",
      "pearson_r: 0.566\n",
      "pearson_r: 0.574\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.06\n",
      "pearson_r: 0.256\n",
      "pearson_r: 0.355\n",
      "pearson_r: 0.401\n",
      "pearson_r: 0.413\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f43f782100f45d6935ceece35239214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ba2907b3c148308ea336f30315a597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.155\n",
      "pearson_r: 0.177\n",
      "pearson_r: 0.203\n",
      "pearson_r: 0.207\n",
      "pearson_r: 0.215\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97529c70-6607-4447-8c2d-06bb551dffcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_freeze: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.243\n",
      "pearson_r: 0.449\n",
      "pearson_r: 0.51\n",
      "pearson_r: 0.553\n",
      "pearson_r: 0.57\n",
      "\n",
      "\n",
      "n_freeze: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.246\n",
      "pearson_r: 0.454\n",
      "pearson_r: 0.51\n",
      "pearson_r: 0.558\n",
      "pearson_r: 0.575\n",
      "\n",
      "\n",
      "n_freeze: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.405\n",
      "pearson_r: 0.48\n",
      "pearson_r: 0.505\n",
      "pearson_r: 0.57\n",
      "pearson_r: 0.585\n",
      "\n",
      "\n",
      "n_freeze: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.398\n",
      "pearson_r: 0.455\n",
      "pearson_r: 0.551\n",
      "pearson_r: 0.612\n",
      "pearson_r: 0.625\n",
      "\n",
      "\n",
      "n_freeze: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.258\n",
      "pearson_r: 0.415\n",
      "pearson_r: 0.531\n",
      "pearson_r: 0.589\n",
      "pearson_r: 0.592\n",
      "\n",
      "\n",
      "n_freeze: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.261\n",
      "pearson_r: 0.446\n",
      "pearson_r: 0.494\n",
      "pearson_r: 0.541\n",
      "pearson_r: 0.582\n",
      "\n",
      "\n",
      "n_freeze: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.212\n",
      "pearson_r: 0.423\n",
      "pearson_r: 0.45\n",
      "pearson_r: 0.493\n",
      "pearson_r: 0.524\n",
      "\n",
      "\n",
      "n_freeze: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.307\n",
      "pearson_r: 0.445\n",
      "pearson_r: 0.477\n",
      "pearson_r: 0.533\n",
      "pearson_r: 0.554\n",
      "\n",
      "\n",
      "n_freeze: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.22\n",
      "pearson_r: 0.41\n",
      "pearson_r: 0.459\n",
      "pearson_r: 0.475\n",
      "pearson_r: 0.485\n",
      "\n",
      "\n",
      "n_freeze: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.195\n",
      "pearson_r: 0.369\n",
      "pearson_r: 0.436\n",
      "pearson_r: 0.473\n",
      "pearson_r: 0.483\n",
      "\n",
      "\n",
      "n_freeze: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.062\n",
      "pearson_r: 0.36\n",
      "pearson_r: 0.458\n",
      "pearson_r: 0.494\n",
      "pearson_r: 0.504\n",
      "\n",
      "\n",
      "n_freeze: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.127\n",
      "pearson_r: 0.268\n",
      "pearson_r: 0.341\n",
      "pearson_r: 0.383\n",
      "pearson_r: 0.394\n",
      "\n",
      "\n",
      "n_freeze: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99d1b7e4004df69e6cf98e1cc802b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f77ca0147464c5ab3e4645ddf556fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.148\n",
      "pearson_r: 0.134\n",
      "pearson_r: 0.158\n",
      "pearson_r: 0.174\n",
      "pearson_r: 0.179\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Earlier without pooling effort\n",
    "for i in range(-1, 12):\n",
    "    print(\"n_freeze:\", i)\n",
    "    train_test(task=task, lr=1e-5, batch_size=8, n_epochs=5, n_freeze=i, seed=1)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
