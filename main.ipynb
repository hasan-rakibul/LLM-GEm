{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d78693-aff6-4484-9689-b92b91844b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_scheduler\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd96ca12-af79-4a12-8f47-218e424f8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"./text-empathy/\")\n",
    "from evaluation import pearsonr\n",
    "from utils.plot import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "561a3d26-7b1e-4458-8dd8-177ee1c49187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623f0a4d-9e54-42e5-bfe3-4c80f728a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['empathy', 'distress']\n",
    "\n",
    "checkpoint = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "train_file = \"./data/PREPROCESSED-essay-train.csv\"\n",
    "dev_file = \"./data/PREPROCESSED-essay-dev.csv\"\n",
    "train_dev_file = \"./data/PREPROCESSED-essay-train-dev.csv\"\n",
    "test_file = \"./data/PREPROCESSED-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97b1bea-0424-437d-a15f-5c10a2fe3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(task, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    trainloader = load_tokenised_data(\n",
    "        filename=train_file, task=task, feature_to_tokenise=features, train=True, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # devloader in train mode to pass labels\n",
    "    devloader = load_tokenised_data(\n",
    "        filename=dev_file, task=task, feature_to_tokenise=features, train=True, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return trainloader, devloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1bc714-7064-4c74-b4eb-35a4bf9f2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(task, features, batch_size):\n",
    "    \"\"\"\n",
    "    feature: list of features to tokenise\n",
    "    \"\"\"\n",
    "    \n",
    "    testloader = load_tokenised_data(\n",
    "        filename=test_file, task=task, feature_to_tokenise=feature, train=False, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1d482d-a1b9-450a-83fa-586a7eaeec03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=3, seed=1, save=False):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = CustomRegressor(n_freeze=0)\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    # print(model)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "              \n",
    "    trainloader, devloader = get_data(task=tasks[0], features=features, batch_size=batch_size)\n",
    "\n",
    "    training_steps = n_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=opt,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=training_steps\n",
    "    )\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    corr = []\n",
    "    for epoch in range(0, n_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss.append(total_loss / len(trainloader)) # len(trainloader) == num_batches\n",
    "\n",
    "        total_loss = 0.0\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch in devloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            batch_pred = [item for sublist in outputs.logits.tolist() for item in sublist]  #convert 2D list to 1D\n",
    "            y_pred.extend(batch_pred)\n",
    "            y_true.extend((batch['labels'].tolist()))\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss.append(total_loss / len(devloader))\n",
    "\n",
    "        # print(y_pred)\n",
    "        corr.append(pearsonr(y_true, y_pred))\n",
    "        print(\"pearson_r:\", corr[-1]) # -1 index refer to the last (current) correlation\n",
    "        \n",
    "        if save and epoch > 0 and (corr[-1] > max(corr[:-1])): #after first epoch, if the last score is greater than max of all but last score\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "            print(\"Saved the model as model.pth in epoch \" + str(epoch+1))\n",
    "            \n",
    "    # train-test finished\n",
    "    plot(\n",
    "        x=list(range(1, n_epochs+1)),\n",
    "        y=train_loss,\n",
    "        y2=val_loss,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='Loss',\n",
    "        legend=['Training loss', 'Validation loss'],\n",
    "        save=False,\n",
    "        filename=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbdb98-0611-4818-b399-60e85173dcf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Demographic as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c13cbfe-6390-49e7-9bca-a147a48e7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['demographic_essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "534441d0-6378-479d-b82f-af1f3abe1316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, task, feature_to_tokenise, train, batch_size, shuffle):\n",
    "    feature_1 = feature_to_tokenise[0]\n",
    "    feature_2 = feature_to_tokenise[1]\n",
    "    \n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_1], sentence[feature_2], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[feature_to_tokenise + [task]]\n",
    "    else:\n",
    "        chosen_data = input_data[feature_to_tokenise]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = feature_to_tokenise)\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task, \"labels\") # as huggingface requires\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "    \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e53f604b-0e21-4091-822c-24165e9fc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(CustomRegressor, self).__init__()\n",
    "\n",
    "        self.transformer_last_hidden_size = 768\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "                num_labels=self.transformer_last_hidden_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(self.transformer_last_hidden_size, 256)\n",
    "        # self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.deberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.deberta.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        # print(outputs.keys())\n",
    "        # will return ['logits', 'hidden_states', 'attentions']\n",
    "\n",
    "        x = self.dropout(outputs.logits) # output.logits aka pooled output from [CLS] token beacuse of AutoModelForSequenceClassification\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        # x = F.tanh(self.fc2(x))\n",
    "        # x = self.dropout(x)\n",
    "        logits = self.fc3(x)\n",
    "                \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "527e60b7-1977-4305-999d-8cd2546d2377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: 0.106\n",
      "pearson_r: 0.227\n",
      "pearson_r: 0.476\n",
      "pearson_r: 0.508\n",
      "pearson_r: 0.505\n",
      "pearson_r: 0.556\n",
      "pearson_r: 0.583\n",
      "pearson_r: 0.555\n",
      "pearson_r: 0.574\n",
      "pearson_r: 0.575\n",
      "pearson_r: 0.57\n",
      "pearson_r: 0.585\n",
      "pearson_r: 0.573\n",
      "pearson_r: 0.59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(task, lr, batch_size, n_epochs, seed, save)\u001b[0m\n\u001b[1;32m     36\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 39\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)) \u001b[38;5;66;03m# len(trainloader) == num_batches\u001b[39;00m\n\u001b[1;32m     43\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=20, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac9b5f-db6b-4fdc-a817-a906d37b0c9d",
   "metadata": {},
   "source": [
    "# Intermediate fusion of numerical demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6c2850-7256-4a9f-9a4b-a14dc296a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['essay', 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9d91bc-af60-458e-806f-88d1877b1119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint, use_fast=False) # fast tokeniser showed warning that it may return incorrect tokens\n",
    "\n",
    "# data collator due to variable max token length per batch size\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokeniser)\n",
    "\n",
    "def load_tokenised_data(filename, task, feature_to_tokenise, train, batch_size, shuffle):\n",
    "    \n",
    "    #padding=\"longest\" can be deferred to do dynamic padding\n",
    "    def tokenise_fn(sentence):\n",
    "        return tokeniser(sentence[feature_to_tokenise[0]], sentence[feature_to_tokenise[1]], truncation=True, max_length=512)\n",
    "   \n",
    "    input_data = pd.read_csv(filename, header=0, index_col=0)\n",
    "\n",
    "    demog = [\"gender\", \"education\", \"race\", \"age\", \"income\"]\n",
    "    \n",
    "    if train:\n",
    "        chosen_data = input_data[feature_to_tokenise + demog + [task]]\n",
    "    else:\n",
    "        chosen_data = input_data[feature_to_tokenise + demog]  #test data shouldn't have output label\n",
    "\n",
    "    hugging_dataset = Dataset.from_pandas(chosen_data, preserve_index=False)\n",
    "\n",
    "    tokenised_hugging_dataset = hugging_dataset.map(tokenise_fn, batched=True, remove_columns = feature_to_tokenise)\n",
    "    \n",
    "    if train:\n",
    "        tokenised_hugging_dataset = tokenised_hugging_dataset.rename_column(task, \"labels\") # as huggingface requires\n",
    "    \n",
    "    tokenised_hugging_dataset = tokenised_hugging_dataset.with_format(\"torch\")\n",
    "    \n",
    "    torchloader = torch.utils.data.DataLoader(\n",
    "        tokenised_hugging_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    return torchloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3471ffd-23c0-41ef-ab5e-fcac6f92bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegressor(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/deberta-v3-base\", n_freeze=-1):\n",
    "        super(CustomRegressor, self).__init__()\n",
    "\n",
    "        self.transformer_last_hidden_size = 768\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                checkpoint,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=True,\n",
    "                num_labels=self.transformer_last_hidden_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # New layers\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(self.transformer_last_hidden_size, 256)\n",
    "        self.fc_demog = nn.Linear(5, 512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(768, 256) #combined layer from two streams. So, the input = sum of the two streams' o/p\n",
    "        self.fc3 = nn.Linear(256, 1) # regression problem\n",
    "\n",
    "        # Freezing layers\n",
    "        if n_freeze:\n",
    "            print(f\"Freezed layers: {n_freeze}\")\n",
    "            # freeze the embedding layer when n_freeze = -1)\n",
    "            for param in self.transformer.deberta.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "            # freeze other layers as per n_freeze\n",
    "            if n_freeze != -1:\n",
    "                for layer in self.transformer.deberta.encoder.layer[:n_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None,\n",
    "        gender = None, education=None, race=None, age=None, income=None\n",
    "    ):\n",
    "        outputs = self.transformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        # print(outputs.keys())\n",
    "        # will return ['logits', 'hidden_states', 'attentions']\n",
    "\n",
    "        x = self.dropout(outputs.logits) # output.logits aka pooled output from [CLS] token beacuse of AutoModelForSequenceClassification\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        demog = torch.stack((gender, education, race, age, income),dim=1)\n",
    "        \n",
    "        x_demog = F.tanh(self.fc_demog(demog))\n",
    "        \n",
    "        x = torch.cat((x, x_demog), dim=1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "                \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89dd5b93-f30b-41c0-a8f0-01bac5717f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_r: -0.009\n",
      "pearson_r: 0.348\n",
      "pearson_r: 0.252\n",
      "pearson_r: 0.341\n",
      "pearson_r: 0.236\n",
      "pearson_r: 0.154\n",
      "pearson_r: 0.262\n",
      "pearson_r: 0.335\n",
      "pearson_r: 0.33\n",
      "pearson_r: 0.349\n",
      "pearson_r: 0.369\n",
      "pearson_r: 0.381\n",
      "pearson_r: 0.391\n",
      "pearson_r: 0.413\n",
      "pearson_r: 0.432\n",
      "pearson_r: 0.414\n",
      "pearson_r: 0.417\n",
      "pearson_r: 0.43\n",
      "pearson_r: 0.429\n",
      "pearson_r: 0.431\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x262.5 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(task=tasks[0], lr=1e-5, batch_size=8, n_epochs=20, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf0ba7c9-a822-40d2-91b1-998e89b5818a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
